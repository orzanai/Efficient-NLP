{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9ab56b27dc0342368ca182920d9a131a",
    "deepnote_cell_type": "code",
    "execution_context_id": "0b41266e-346f-44a4-8464-25a0535df017",
    "execution_millis": 4986,
    "execution_start": 1757419031141,
    "source_hash": "e4a26018"
   },
   "outputs": [],
   "source": [
    "# Install/upgrade trainer deps\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ca064b0e4cc9494ba6aaf04ca30fed07",
    "deepnote_cell_type": "code",
    "execution_context_id": "36ec1973-ad65-4964-8f93-960d5b34837a",
    "execution_millis": 8573,
    "execution_start": 1757409913851,
    "source_hash": "26059997"
   },
   "outputs": [],
   "source": [
    "# Imports & helpers\n",
    "\n",
    "import os, random, numpy as np, pandas as pd, torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from codecarbon import EmissionsTracker\n",
    "import random as _rnd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "\n",
    "\n",
    "def get_energy_kwh(tracker) -> float:\n",
    "    data = getattr(tracker, \"final_emissions_data\", None) or getattr(tracker, \"_emissions_data\", None)\n",
    "    if data is not None:\n",
    "        energy = getattr(data, \"energy_consumed\", None)\n",
    "        if energy is not None:\n",
    "            return float(energy)\n",
    "        parts = [getattr(data, \"cpu_energy\", 0.0),\n",
    "                 getattr(data, \"gpu_energy\", 0.0),\n",
    "                 getattr(data, \"ram_energy\", 0.0)]\n",
    "        if any(parts):\n",
    "            return float(sum(p for p in parts if p is not None))\n",
    "    return 0.0\n",
    "\n",
    "def _stratified_cap_split(split_ds, label_field, max_n, seed=42):\n",
    "    if max_n is None or max_n >= len(split_ds):\n",
    "        return split_ds\n",
    "    labels = split_ds[label_field]\n",
    "    idx_by_label = {}\n",
    "    for i, y in enumerate(labels):\n",
    "        idx_by_label.setdefault(int(y), []).append(i)\n",
    "    rng = _rnd.Random(seed)\n",
    "    for y in idx_by_label: rng.shuffle(idx_by_label[y])\n",
    "    num_labels = len(idx_by_label)\n",
    "    base = max_n // num_labels\n",
    "    rem = max_n % num_labels\n",
    "    selected = []\n",
    "    for k, y in enumerate(sorted(idx_by_label.keys())):\n",
    "        take = min(base + (1 if k < rem else 0), len(idx_by_label[y]))\n",
    "        selected += idx_by_label[y][:take]\n",
    "    if len(selected) < max_n:\n",
    "        remaining = []\n",
    "        for k, y in enumerate(sorted(idx_by_label.keys())):\n",
    "            start = base + (1 if k < rem else 0)\n",
    "            remaining += idx_by_label[y][start:]\n",
    "        rng.shuffle(remaining)\n",
    "        need = max_n - len(selected)\n",
    "        selected += remaining[:need]\n",
    "    selected = selected[:max_n]\n",
    "    selected.sort()\n",
    "    return split_ds.select(selected)\n",
    "\n",
    "def cap_dataset_stratified(ds: DatasetDict, label_field: str, max_train=None, max_test=None, seed=42) -> DatasetDict:\n",
    "    out = {}\n",
    "    for split in ds.keys():\n",
    "        if split == \"train\":\n",
    "            out[split] = _stratified_cap_split(ds[split], label_field, max_train, seed)\n",
    "        elif split == \"test\":\n",
    "            out[split] = _stratified_cap_split(ds[split], label_field, max_test, seed)\n",
    "        else:\n",
    "            out[split] = ds[split]\n",
    "    return DatasetDict(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "402f96cdd3f944abb8d81ef255cfce1f",
    "deepnote_cell_type": "code",
    "execution_context_id": "36ec1973-ad65-4964-8f93-960d5b34837a",
    "execution_millis": 12586,
    "execution_start": 1757409924902,
    "source_hash": "55615871"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 3600000/3600000 [00:05<00:00, 630483.46 examples/s]\n",
      "Generating test split: 100%|██████████| 400000/400000 [00:00<00:00, 676839.88 examples/s]\n",
      "Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1138686.42 examples/s]\n",
      "Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 846052.24 examples/s]\n",
      "Generating train split: 100%|██████████| 560000/560000 [00:00<00:00, 974044.22 examples/s] \n",
      "Generating test split: 100%|██████████| 70000/70000 [00:00<00:00, 982966.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load datasets (Amazon, AG News, DBpedia)\n",
    "\n",
    "raw_datasets = {\n",
    "    \"amazon\":  load_dataset(\"amazon_polarity\"),\n",
    "    \"ag_news\": load_dataset(\"ag_news\"),\n",
    "    \"dbpedia\": load_dataset(\"dbpedia_14\"),\n",
    "}\n",
    "\n",
    "DATA_META = {\n",
    "    \"amazon\":  {\"text\": \"content\", \"label\": \"label\", \"num_labels\": 2},\n",
    "    \"ag_news\": {\"text\": \"text\",    \"label\": \"label\", \"num_labels\": 4},\n",
    "    \"dbpedia\": {\"text\": \"content\", \"label\": \"label\", \"num_labels\": 14},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "15d1d1bfb733418a9e3e11b8c08eba64",
    "deepnote_cell_type": "code",
    "execution_context_id": "68020ee9-7d3b-4fb5-ac0d-50d5c2e5a162",
    "execution_millis": 0,
    "execution_start": 1756210837085,
    "source_hash": "3e0bcdc2"
   },
   "outputs": [],
   "source": [
    "# TF-IDF baselines\n",
    "\n",
    "def run_tfidf_experiment(name, dataset, text_field, label_field, model_type=\"logreg\"):\n",
    "    train_texts = dataset[\"train\"][text_field]\n",
    "    train_labels = dataset[\"train\"][label_field]\n",
    "    test_texts  = dataset[\"test\"][text_field]\n",
    "    test_labels = dataset[\"test\"][label_field]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2), stop_words=\"english\")\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_test  = vectorizer.transform(test_texts)\n",
    "\n",
    "    tracker = EmissionsTracker(\n",
    "        project_name=f\"TFIDF_{model_type.upper()}_{name.upper()}\",\n",
    "        measure_power_secs=1, save_to_file=False, log_level=\"error\"\n",
    "    )\n",
    "    tracker.start()\n",
    "\n",
    "    if model_type == \"logreg\":\n",
    "        model = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
    "    elif model_type == \"svm\":\n",
    "        model = LinearSVC()\n",
    "    elif model_type == \"cnb\":\n",
    "        model = ComplementNB()\n",
    "    else:\n",
    "        raise ValueError(\"model_type ∈ {'logreg','svm','cnb'}\")\n",
    "\n",
    "    model.fit(X_train, train_labels)\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    emissions = tracker.stop()\n",
    "    energy_kwh = get_energy_kwh(tracker)\n",
    "    acc = accuracy_score(test_labels, preds)\n",
    "    print(f\"[TF-IDF + {model_type.upper()}] {name}  Acc: {acc:.4f}  CO₂: {emissions:.6f} kg  Energy: {energy_kwh:.6f} kWh\")\n",
    "\n",
    "    return {\n",
    "        \"model\": f\"TF-IDF+{model_type.upper()}\",\n",
    "        \"dataset\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"emissions\": emissions,\n",
    "        \"energy_kwh\": energy_kwh,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "f2f61d916c414c5fafe58f04c97f024d",
    "deepnote_cell_type": "code",
    "execution_context_id": "68020ee9-7d3b-4fb5-ac0d-50d5c2e5a162",
    "execution_millis": 3,
    "execution_start": 1756210842500,
    "source_hash": "6824a6b9"
   },
   "outputs": [],
   "source": [
    "# BERT-base runner (CUDA, AMP; tuned for T4)\n",
    "\n",
    "def run_bert_experiment(name, dataset, text_field, label_field, num_labels, num_epochs=2):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(examples[text_field], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    encoded = dataset.map(\n",
    "        tokenize_fn, batched=True,\n",
    "        remove_columns=[c for c in dataset[\"train\"].column_names if c not in {text_field, label_field}]\n",
    "    )\n",
    "    encoded = encoded.rename_column(label_field, \"labels\")\n",
    "    encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./results_{name}\",\n",
    "        eval_strategy=\"epoch\",  # transformers 4.55.x\n",
    "        per_device_train_batch_size=16,           \n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=num_epochs,              \n",
    "        gradient_accumulation_steps=1,\n",
    "        dataloader_num_workers=2,\n",
    "        dataloader_pin_memory=True,\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        logging_dir=f\"./logs_{name}\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        fp16=torch.cuda.is_available(),           \n",
    "        no_cuda=False,\n",
    "        torch_compile=False,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        if isinstance(preds, tuple): preds = preds[0]\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "        return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "    print(\"Trainer device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=encoded[\"train\"],\n",
    "        eval_dataset=encoded[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    tracker = EmissionsTracker(\n",
    "        project_name=f\"BERT_{name.upper()}\",\n",
    "        measure_power_secs=1, save_to_file=False, log_level=\"error\"\n",
    "    )\n",
    "    tracker.start()\n",
    "    trainer.train()\n",
    "    emissions = tracker.stop()\n",
    "    energy_kwh = get_energy_kwh(tracker)\n",
    "\n",
    "    acc = trainer.evaluate()[\"eval_accuracy\"]\n",
    "    print(f\"[BERT] {name}  Acc: {acc:.4f}  CO₂: {emissions:.6f} kg  Energy: {energy_kwh:.6f} kWh\")\n",
    "\n",
    "    return {\n",
    "        \"model\": \"BERT-base\",\n",
    "        \"dataset\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"emissions\": emissions,\n",
    "        \"energy_kwh\": energy_kwh,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_id": "938c5ca96442499b8bbae4b716d0fca7",
    "deepnote_cell_type": "code",
    "execution_context_id": "68020ee9-7d3b-4fb5-ac0d-50d5c2e5a162",
    "execution_millis": 26161937,
    "execution_start": 1756210853509,
    "source_hash": "1da1f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 08:23:26] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[TF-IDF + LOGREG] amazon  Acc: 0.8764  CO₂: 0.000069 kg  Energy: 0.000186 kWh\n",
      "[TF-IDF + SVM] amazon  Acc: 0.8763  CO₂: 0.000170 kg  Energy: 0.000460 kWh\n",
      "[TF-IDF + CNB] amazon  Acc: 0.8380  CO₂: 0.000064 kg  Energy: 0.000174 kWh\n",
      "Map: 100%|██████████| 400000/400000 [01:11<00:00, 5581.22 examples/s]\n",
      "Map: 100%|██████████| 40000/40000 [00:07<00:00, 5670.57 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Trainer device: cuda\n",
      "/tmp/ipykernel_75/549541287.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/50000 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   1/1250 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] amazon  Acc: 0.9478  CO₂: 0.073466 kg  Energy: 0.199022 kWh\n",
      "[TF-IDF + LOGREG] ag_news  Acc: 0.9149  CO₂: 0.000072 kg  Energy: 0.000196 kWh\n",
      "[TF-IDF + SVM] ag_news  Acc: 0.9164  CO₂: 0.000088 kg  Energy: 0.000240 kWh\n",
      "[TF-IDF + CNB] ag_news  Acc: 0.8987  CO₂: 0.000014 kg  Energy: 0.000038 kWh\n",
      "Map: 100%|██████████| 120000/120000 [00:15<00:00, 7882.69 examples/s]\n",
      "Map: 100%|██████████| 7600/7600 [00:00<00:00, 8636.44 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_75/549541287.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Trainer device: cuda\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/15000 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/238 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] ag_news  Acc: 0.9501  CO₂: 0.021607 kg  Energy: 0.058533 kWh\n",
      "[TF-IDF + LOGREG] dbpedia  Acc: 0.9779  CO₂: 0.000294 kg  Energy: 0.000797 kWh\n",
      "[TF-IDF + SVM] dbpedia  Acc: 0.9783  CO₂: 0.000483 kg  Energy: 0.001308 kWh\n",
      "[TF-IDF + CNB] dbpedia  Acc: 0.9377  CO₂: 0.000047 kg  Energy: 0.000126 kWh\n",
      "Map: 100%|██████████| 560000/560000 [01:13<00:00, 7648.26 examples/s]\n",
      "Map: 100%|██████████| 70000/70000 [00:09<00:00, 7680.24 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_75/549541287.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Trainer device: cuda\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='70000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/70000 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='2188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   1/2188 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] dbpedia  Acc: 0.9923  CO₂: 0.102488 kg  Energy: 0.277645 kWh\n"
     ]
    },
    {
     "data": {
      "application/vnd.deepnote.dataframe.v3+json": {
       "column_count": 5,
       "columns": [
        {
         "dtype": "object",
         "name": "model",
         "stats": {
          "categories": [
           {
            "count": 3,
            "name": "TF-IDF+LOGREG"
           },
           {
            "count": 3,
            "name": "TF-IDF+SVM"
           },
           {
            "count": 6,
            "name": "2 others"
           }
          ],
          "histogram": null,
          "max": null,
          "min": null,
          "nan_count": 0,
          "unique_count": 4
         }
        },
        {
         "dtype": "object",
         "name": "dataset",
         "stats": {
          "categories": [
           {
            "count": 4,
            "name": "amazon"
           },
           {
            "count": 4,
            "name": "ag_news"
           },
           {
            "count": 4,
            "name": "dbpedia"
           }
          ],
          "histogram": null,
          "max": null,
          "min": null,
          "nan_count": 0,
          "unique_count": 3
         }
        },
        {
         "dtype": "float64",
         "name": "accuracy",
         "stats": {
          "categories": null,
          "histogram": [
           {
            "bin_end": 0.8534721428571428,
            "bin_start": 0.83805,
            "count": 1
           },
           {
            "bin_end": 0.8688942857142857,
            "bin_start": 0.8534721428571428,
            "count": 0
           },
           {
            "bin_end": 0.8843164285714286,
            "bin_start": 0.8688942857142857,
            "count": 2
           },
           {
            "bin_end": 0.8997385714285714,
            "bin_start": 0.8843164285714286,
            "count": 1
           },
           {
            "bin_end": 0.9151607142857143,
            "bin_start": 0.8997385714285714,
            "count": 1
           },
           {
            "bin_end": 0.9305828571428572,
            "bin_start": 0.9151607142857143,
            "count": 1
           },
           {
            "bin_end": 0.946005,
            "bin_start": 0.9305828571428572,
            "count": 1
           },
           {
            "bin_end": 0.9614271428571428,
            "bin_start": 0.946005,
            "count": 2
           },
           {
            "bin_end": 0.9768492857142858,
            "bin_start": 0.9614271428571428,
            "count": 0
           },
           {
            "bin_end": 0.9922714285714286,
            "bin_start": 0.9768492857142858,
            "count": 3
           }
          ],
          "max": "0.9922714285714286",
          "min": "0.83805",
          "nan_count": 0,
          "unique_count": 12
         }
        },
        {
         "dtype": "float64",
         "name": "emissions",
         "stats": {
          "categories": null,
          "histogram": [
           {
            "bin_end": 0.010261390260431833,
            "bin_start": 0.000013951906954813969,
            "count": 9
           },
           {
            "bin_end": 0.020508828613908854,
            "bin_start": 0.010261390260431833,
            "count": 0
           },
           {
            "bin_end": 0.030756266967385874,
            "bin_start": 0.020508828613908854,
            "count": 1
           },
           {
            "bin_end": 0.04100370532086289,
            "bin_start": 0.030756266967385874,
            "count": 0
           },
           {
            "bin_end": 0.051251143674339916,
            "bin_start": 0.04100370532086289,
            "count": 0
           },
           {
            "bin_end": 0.06149858202781693,
            "bin_start": 0.051251143674339916,
            "count": 0
           },
           {
            "bin_end": 0.07174602038129395,
            "bin_start": 0.06149858202781693,
            "count": 0
           },
           {
            "bin_end": 0.08199345873477097,
            "bin_start": 0.07174602038129395,
            "count": 1
           },
           {
            "bin_end": 0.092240897088248,
            "bin_start": 0.08199345873477097,
            "count": 0
           },
           {
            "bin_end": 0.102488335441725,
            "bin_start": 0.092240897088248,
            "count": 1
           }
          ],
          "max": "0.102488335441725",
          "min": "1.3951906954813969e-05",
          "nan_count": 0,
          "unique_count": 12
         }
        },
        {
         "dtype": "float64",
         "name": "energy_kwh",
         "stats": {
          "categories": null,
          "histogram": [
           {
            "bin_end": 0.02779852236756204,
            "bin_start": 0.000037796281762040574,
            "count": 9
           },
           {
            "bin_end": 0.05555924845336204,
            "bin_start": 0.02779852236756204,
            "count": 0
           },
           {
            "bin_end": 0.08331997453916204,
            "bin_start": 0.05555924845336204,
            "count": 1
           },
           {
            "bin_end": 0.11108070062496203,
            "bin_start": 0.08331997453916204,
            "count": 0
           },
           {
            "bin_end": 0.13884142671076202,
            "bin_start": 0.11108070062496203,
            "count": 0
           },
           {
            "bin_end": 0.16660215279656204,
            "bin_start": 0.13884142671076202,
            "count": 0
           },
           {
            "bin_end": 0.19436287888236203,
            "bin_start": 0.16660215279656204,
            "count": 0
           },
           {
            "bin_end": 0.22212360496816203,
            "bin_start": 0.19436287888236203,
            "count": 1
           },
           {
            "bin_end": 0.24988433105396202,
            "bin_start": 0.22212360496816203,
            "count": 0
           },
           {
            "bin_end": 0.277645057139762,
            "bin_start": 0.24988433105396202,
            "count": 1
           }
          ],
          "max": "0.277645057139762",
          "min": "3.7796281762040574e-05",
          "nan_count": 0,
          "unique_count": 12
         }
        },
        {
         "dtype": "int64",
         "name": "_deepnote_index_column"
        }
       ],
       "preview_row_count": 12,
       "row_count": 12,
       "rows": [
        {
         "_deepnote_index_column": 0,
         "accuracy": 0.8764,
         "dataset": "amazon",
         "emissions": 0.00006856831167350798,
         "energy_kwh": 0.00018575433712056887,
         "model": "TF-IDF+LOGREG"
        },
        {
         "_deepnote_index_column": 1,
         "accuracy": 0.8763,
         "dataset": "amazon",
         "emissions": 0.00016974310189318606,
         "energy_kwh": 0.0004598409469828039,
         "model": "TF-IDF+SVM"
        },
        {
         "_deepnote_index_column": 2,
         "accuracy": 0.83805,
         "dataset": "amazon",
         "emissions": 0.00006432596228426539,
         "energy_kwh": 0.00017426164057606473,
         "model": "TF-IDF+CNB"
        },
        {
         "_deepnote_index_column": 3,
         "accuracy": 0.94785,
         "dataset": "amazon",
         "emissions": 0.07346598312404264,
         "energy_kwh": 0.1990223277057869,
         "model": "BERT-base"
        },
        {
         "_deepnote_index_column": 4,
         "accuracy": 0.9148684210526316,
         "dataset": "ag_news",
         "emissions": 0.00007249092158004209,
         "energy_kwh": 0.0001963808464393383,
         "model": "TF-IDF+LOGREG"
        },
        {
         "_deepnote_index_column": 5,
         "accuracy": 0.9164473684210527,
         "dataset": "ag_news",
         "emissions": 0.00008845443584472989,
         "energy_kwh": 0.00023962665398483016,
         "model": "TF-IDF+SVM"
        },
        {
         "_deepnote_index_column": 6,
         "accuracy": 0.8986842105263158,
         "dataset": "ag_news",
         "emissions": 0.000013951906954813969,
         "energy_kwh": 0.000037796281762040574,
         "model": "TF-IDF+CNB"
        },
        {
         "_deepnote_index_column": 7,
         "accuracy": 0.9501315789473684,
         "dataset": "ag_news",
         "emissions": 0.021606624049699725,
         "energy_kwh": 0.05853322081015999,
         "model": "BERT-base"
        },
        {
         "_deepnote_index_column": 8,
         "accuracy": 0.9779285714285715,
         "dataset": "dbpedia",
         "emissions": 0.00029402929341036156,
         "energy_kwh": 0.000796537280245925,
         "model": "TF-IDF+LOGREG"
        },
        {
         "_deepnote_index_column": 9,
         "accuracy": 0.9783142857142857,
         "dataset": "dbpedia",
         "emissions": 0.0004827877213638346,
         "energy_kwh": 0.0013078915166951338,
         "model": "TF-IDF+SVM"
        }
       ],
       "type": "dataframe"
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>emissions</th>\n",
       "      <th>energy_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF+LOGREG</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.876300</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF-IDF+CNB</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.838050</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BERT-base</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.947850</td>\n",
       "      <td>0.073466</td>\n",
       "      <td>0.199022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TF-IDF+LOGREG</td>\n",
       "      <td>ag_news</td>\n",
       "      <td>0.914868</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>ag_news</td>\n",
       "      <td>0.916447</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TF-IDF+CNB</td>\n",
       "      <td>ag_news</td>\n",
       "      <td>0.898684</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BERT-base</td>\n",
       "      <td>ag_news</td>\n",
       "      <td>0.950132</td>\n",
       "      <td>0.021607</td>\n",
       "      <td>0.058533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TF-IDF+LOGREG</td>\n",
       "      <td>dbpedia</td>\n",
       "      <td>0.977929</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>dbpedia</td>\n",
       "      <td>0.978314</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.001308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TF-IDF+CNB</td>\n",
       "      <td>dbpedia</td>\n",
       "      <td>0.937686</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BERT-base</td>\n",
       "      <td>dbpedia</td>\n",
       "      <td>0.992271</td>\n",
       "      <td>0.102488</td>\n",
       "      <td>0.277645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  dataset  accuracy  emissions  energy_kwh\n",
       "0   TF-IDF+LOGREG   amazon  0.876400   0.000069    0.000186\n",
       "1      TF-IDF+SVM   amazon  0.876300   0.000170    0.000460\n",
       "2      TF-IDF+CNB   amazon  0.838050   0.000064    0.000174\n",
       "3       BERT-base   amazon  0.947850   0.073466    0.199022\n",
       "4   TF-IDF+LOGREG  ag_news  0.914868   0.000072    0.000196\n",
       "5      TF-IDF+SVM  ag_news  0.916447   0.000088    0.000240\n",
       "6      TF-IDF+CNB  ag_news  0.898684   0.000014    0.000038\n",
       "7       BERT-base  ag_news  0.950132   0.021607    0.058533\n",
       "8   TF-IDF+LOGREG  dbpedia  0.977929   0.000294    0.000797\n",
       "9      TF-IDF+SVM  dbpedia  0.978314   0.000483    0.001308\n",
       "10     TF-IDF+CNB  dbpedia  0.937686   0.000047    0.000126\n",
       "11      BERT-base  dbpedia  0.992271   0.102488    0.277645"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprehensive experiment\n",
    "\n",
    "AMAZON_CAP_TRAIN = 400_000  \n",
    "AMAZON_CAP_TEST  = 40_000\n",
    "\n",
    "results = []\n",
    "model_types = [\"logreg\", \"svm\", \"cnb\"]\n",
    "\n",
    "for dataset_key, meta in {\n",
    "    \"amazon\":  {\"text\": \"content\", \"label\": \"label\", \"num_labels\": 2},\n",
    "    \"ag_news\": {\"text\": \"text\",    \"label\": \"label\", \"num_labels\": 4},\n",
    "    \"dbpedia\": {\"text\": \"content\", \"label\": \"label\", \"num_labels\": 14},\n",
    "}.items():\n",
    "    if dataset_key == \"amazon\":\n",
    "        data = cap_dataset_stratified(raw_datasets[\"amazon\"], meta[\"label\"],\n",
    "                                      max_train=AMAZON_CAP_TRAIN, max_test=AMAZON_CAP_TEST, seed=SEED)\n",
    "    else:\n",
    "        data = raw_datasets[dataset_key]\n",
    "\n",
    "    # TF-IDF trio\n",
    "    for m in model_types:\n",
    "        results.append(run_tfidf_experiment(dataset_key, data, meta[\"text\"], meta[\"label\"], model_type=m))\n",
    "\n",
    "    # BERT-base (CUDA+AMP) \n",
    "    results.append(run_bert_experiment(dataset_key, data, meta[\"text\"], meta[\"label\"], meta[\"num_labels\"], num_epochs=2))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"model_comparison_report.csv\", index=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "99e83fe3dc8842caaa049a6ad2db9bc2",
    "deepnote_cell_type": "code",
    "execution_context_id": "0b41266e-346f-44a4-8464-25a0535df017",
    "execution_millis": 4102,
    "execution_start": 1757418764891,
    "source_hash": "82dc23fb"
   },
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "\n",
    "df = pd.read_csv(\"model_comparison_report.csv\")\n",
    "\n",
    "model_order = [\"TF-IDF+LOGREG\", \"TF-IDF+SVM\", \"TF-IDF+CNB\", \"BERT-base\"]\n",
    "colors = {\"TF-IDF+LOGREG\":\"#1f77b4\",\"TF-IDF+SVM\":\"#ff7f0e\",\"TF-IDF+CNB\":\"#2ca02c\",\"BERT-base\":\"#d62728\"}\n",
    "markers = {\"TF-IDF+LOGREG\":\"o\",\"TF-IDF+SVM\":\"s\",\"TF-IDF+CNB\":\"^\",\"BERT-base\":\"X\"}\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "df_sorted = df.set_index(\"model\").loc[model_order].reset_index()\n",
    "for dataset in df_sorted[\"dataset\"].unique():\n",
    "    sub = df_sorted[df_sorted[\"dataset\"]==dataset]\n",
    "    plt.bar(sub[\"model\"], sub[\"accuracy\"], color=[colors[m] for m in sub[\"model\"]])\n",
    "plt.title(\"Accuracy by Model and Dataset\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(model_order, bbox_to_anchor=(1.05,1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"accuracy_by_model_grouped.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "datasets = df[\"dataset\"].unique()\n",
    "fig, axes = plt.subplots(1, len(datasets), figsize=(18,6), sharey=True)\n",
    "for ax,dataset in zip(axes,datasets):\n",
    "    sub = df[df[\"dataset\"]==dataset]\n",
    "    for m in model_order:\n",
    "        row = sub[sub[\"model\"]==m]\n",
    "        if not row.empty:\n",
    "            ax.scatter(row[\"accuracy\"], row[\"energy_kwh\"], color=colors[m], marker=markers[m], s=100, label=m)\n",
    "            ax.text(row[\"accuracy\"].values[0], row[\"energy_kwh\"].values[0], m.split(\"+\")[-1], fontsize=8, ha=\"center\", va=\"bottom\")\n",
    "    ax.set_title(dataset)\n",
    "    ax.set_xlabel(\"Accuracy\")\n",
    "    ax.set_yscale(\"log\")\n",
    "axes[0].set_ylabel(\"Energy (kWh, log scale)\")\n",
    "handles = [plt.Line2D([0],[0], marker=markers[m], color='w', markerfacecolor=colors[m], markersize=10, label=m) for m in model_order]\n",
    "fig.legend(handles=handles, labels=model_order, loc=\"upper center\", ncol=4)\n",
    "fig.suptitle(\"Accuracy vs Energy (kWh, log scale) — Models per Dataset\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"accuracy_vs_energy_by_model_faceted_log.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "df_sorted = df.set_index(\"model\").loc[model_order].reset_index()\n",
    "for dataset in df_sorted[\"dataset\"].unique():\n",
    "    sub = df_sorted[df_sorted[\"dataset\"]==dataset]\n",
    "    plt.bar(sub[\"model\"], sub[\"energy_kwh\"], color=[colors[m] for m in sub[\"model\"]])\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Energy (kWh) by Model and Dataset\")\n",
    "plt.ylabel(\"Energy (kWh, log scale)\")\n",
    "plt.legend(model_order, bbox_to_anchor=(1.05,1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"energy_by_model_grouped.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "for dataset in df[\"dataset\"].unique():\n",
    "    sub = df[df[\"dataset\"]==dataset]\n",
    "    n_test = sub[\"accuracy\"].values[0]*10000\n",
    "    sub[\"energy_per_correct\"] = sub[\"energy_kwh\"]/(sub[\"accuracy\"]*n_test)\n",
    "    plt.bar([f\"{dataset}-{m}\" for m in sub[\"model\"]], sub[\"energy_per_correct\"], color=[colors[m] for m in sub[\"model\"]])\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Energy per Correct Prediction by Model within Dataset\")\n",
    "plt.ylabel(\"kWh per Correct Prediction (log scale)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"F3_kwh_per_correct.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "rows = []\n",
    "for dataset in df[\"dataset\"].unique():\n",
    "    sub = df[df[\"dataset\"]==dataset]\n",
    "    tfidf_best = sub[sub[\"model\"].str.contains(\"TF-IDF\")].sort_values(\"accuracy\", ascending=False).iloc[0]\n",
    "    bert = sub[sub[\"model\"]==\"BERT-base\"].iloc[0]\n",
    "    rows.append({\n",
    "        \"dataset\":dataset,\n",
    "        \"delta_acc\": bert[\"accuracy\"]-tfidf_best[\"accuracy\"],\n",
    "        \"delta_energy\": bert[\"energy_kwh\"]-tfidf_best[\"energy_kwh\"]\n",
    "    })\n",
    "comp = pd.DataFrame(rows)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6),sharey=True)\n",
    "axes[0].barh(comp[\"dataset\"], comp[\"delta_acc\"])\n",
    "axes[0].set_xlabel(\"ΔAccuracy (BERT – best TF-IDF)\")\n",
    "axes[0].set_title(\"Accuracy Gain of BERT vs best TF-IDF\")\n",
    "axes[1].barh(comp[\"dataset\"], comp[\"delta_energy\"])\n",
    "axes[1].set_xlabel(\"ΔEnergy (kWh) (BERT – best TF-IDF)\")\n",
    "axes[1].set_title(\"Energy Overhead of BERT vs best TF-IDF\")\n",
    "fig.suptitle(\"BERT vs best TF-IDF — Accuracy Gain vs Energy Overhead\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"F5_tfidf_vs_bert_deltas.png\", dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency table (BERT vs best TF-IDF per dataset)\n",
    "\n",
    "J_PER_KWH = 3_600_000.0\n",
    "\n",
    "def _safe_div(a, b, default=np.nan):\n",
    "    try:\n",
    "        return a / b if b != 0 else default\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "TEST_SIZES = {\n",
    "    \"amazon\": 40000,\n",
    "    \"ag_news\": len(raw_datasets[\"ag_news\"][\"test\"]),\n",
    "    \"dbpedia\": len(raw_datasets[\"dbpedia\"][\"test\"]),\n",
    "}\n",
    "\n",
    "def compute_efficiency(df):\n",
    "    rows = []\n",
    "    for dataset in df[\"dataset\"].unique():\n",
    "        sub = df[df[\"dataset\"] == dataset]\n",
    "        tfidf_best = sub[sub[\"model\"].str.contains(\"TF-IDF\")].sort_values(\"accuracy\", ascending=False).iloc[0]\n",
    "        bert = sub[sub[\"model\"] == \"BERT-base\"].iloc[0]\n",
    "\n",
    "        n_test = TEST_SIZES.get(dataset, np.nan)\n",
    "        delta_acc = float(bert[\"accuracy\"] - tfidf_best[\"accuracy\"])\n",
    "        delta_energy_kwh = float(bert[\"energy_kwh\"] - tfidf_best[\"energy_kwh\"])\n",
    "        joules_per_1pct = _safe_div(delta_energy_kwh * J_PER_KWH, (delta_acc * 100.0), default=np.inf)\n",
    "\n",
    "        rows.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"best_tfidf_model\": tfidf_best[\"model\"],\n",
    "            \"tfidf_accuracy\": float(tfidf_best[\"accuracy\"]),\n",
    "            \"bert_accuracy\": float(bert[\"accuracy\"]),\n",
    "            \"delta_accuracy\": delta_acc,\n",
    "            \"tfidf_energy_kwh\": float(tfidf_best[\"energy_kwh\"]),\n",
    "            \"bert_energy_kwh\": float(bert[\"energy_kwh\"]),\n",
    "            \"delta_energy_kwh\": delta_energy_kwh,\n",
    "            \"tfidf_co2_kg\": float(tfidf_best[\"emissions\"]),\n",
    "            \"bert_co2_kg\": float(bert[\"emissions\"]),\n",
    "            \"tfidf_energy_per_test_ex_kWh\": _safe_div(float(tfidf_best[\"energy_kwh\"]), n_test),\n",
    "            \"bert_energy_per_test_ex_kWh\": _safe_div(float(bert[\"energy_kwh\"]), n_test),\n",
    "            \"joules_per_1pct_acc\": joules_per_1pct,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "efficiency_df = compute_efficiency(results_df)\n",
    "efficiency_df.to_csv(\"energy_efficiency_report.csv\", index=False)\n",
    "with open(\"energy_efficiency_report.tex\",\"w\") as f:\n",
    "    f.write(efficiency_df.to_latex(index=False, float_format=\"%.6g\"))\n",
    "efficiency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "0f203196697a4be698eaf32b04d84831",
    "deepnote_cell_type": "code",
    "execution_context_id": "c3e9c748-36fe-401f-bb64-5c507344e556",
    "execution_millis": 771,
    "execution_start": 1757419647013,
    "source_hash": "37006ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - metrics_deltas_bert_vs_best_tfidf.csv\n",
      " - metrics_joules_per_1pct_all_models_vs_best_tfidf.csv\n",
      " - metrics_energy_normalized.csv\n",
      " - metrics_best_tfidf_per_dataset.csv\n",
      "\n",
      "Preview — deltas (BERT vs best TF-IDF):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>ref_tfidf_model</th>\n",
       "      <th>bert_accuracy</th>\n",
       "      <th>ref_accuracy</th>\n",
       "      <th>delta_accuracy</th>\n",
       "      <th>bert_energy_kwh</th>\n",
       "      <th>ref_energy_kwh</th>\n",
       "      <th>delta_energy_kwh</th>\n",
       "      <th>bert_co2_kg</th>\n",
       "      <th>ref_co2_kg</th>\n",
       "      <th>delta_co2_kg</th>\n",
       "      <th>joules_per_1pct_acc_bert_vs_best_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ag_news</td>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>0.950132</td>\n",
       "      <td>0.916447</td>\n",
       "      <td>0.033684</td>\n",
       "      <td>0.058533</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.058294</td>\n",
       "      <td>0.021607</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>62301.278754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon</td>\n",
       "      <td>TF-IDF+LOGREG</td>\n",
       "      <td>0.947850</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.199022</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.198837</td>\n",
       "      <td>0.073466</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.073397</td>\n",
       "      <td>100183.577904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbpedia</td>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>0.992271</td>\n",
       "      <td>0.978314</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.277645</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.276337</td>\n",
       "      <td>0.102488</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.102006</td>\n",
       "      <td>712763.211228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset ref_tfidf_model  bert_accuracy  ref_accuracy  delta_accuracy  \\\n",
       "0  ag_news      TF-IDF+SVM       0.950132      0.916447        0.033684   \n",
       "1   amazon   TF-IDF+LOGREG       0.947850      0.876400        0.071450   \n",
       "2  dbpedia      TF-IDF+SVM       0.992271      0.978314        0.013957   \n",
       "\n",
       "   bert_energy_kwh  ref_energy_kwh  delta_energy_kwh  bert_co2_kg  ref_co2_kg  \\\n",
       "0         0.058533        0.000240          0.058294     0.021607    0.000088   \n",
       "1         0.199022        0.000186          0.198837     0.073466    0.000069   \n",
       "2         0.277645        0.001308          0.276337     0.102488    0.000483   \n",
       "\n",
       "   delta_co2_kg  joules_per_1pct_acc_bert_vs_best_tfidf  \n",
       "0      0.021518                            62301.278754  \n",
       "1      0.073397                           100183.577904  \n",
       "2      0.102006                           712763.211228  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview — J per +1% accuracy (all models vs best TF-IDF):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>ref_tfidf_model</th>\n",
       "      <th>model_accuracy</th>\n",
       "      <th>ref_accuracy</th>\n",
       "      <th>delta_accuracy</th>\n",
       "      <th>model_energy_kwh</th>\n",
       "      <th>ref_energy_kwh</th>\n",
       "      <th>delta_energy_kwh</th>\n",
       "      <th>joules_per_1pct_acc_vs_best_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ag_news</td>\n",
       "      <td>BERT-base</td>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>0.950132</td>\n",
       "      <td>0.916447</td>\n",
       "      <td>0.033684</td>\n",
       "      <td>0.058533</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.058294</td>\n",
       "      <td>62301.278754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ag_news</td>\n",
       "      <td>TF-IDF+CNB</td>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>0.898684</td>\n",
       "      <td>0.916447</td>\n",
       "      <td>-0.017763</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>409.042888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ag_news</td>\n",
       "      <td>TF-IDF+LOGREG</td>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>0.914868</td>\n",
       "      <td>0.916447</td>\n",
       "      <td>-0.001579</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>986.004412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ag_news</td>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>0.916447</td>\n",
       "      <td>0.916447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon</td>\n",
       "      <td>BERT-base</td>\n",
       "      <td>TF-IDF+LOGREG</td>\n",
       "      <td>0.947850</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.199022</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.198837</td>\n",
       "      <td>100183.577904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset          model ref_tfidf_model  model_accuracy  ref_accuracy  \\\n",
       "0  ag_news      BERT-base      TF-IDF+SVM        0.950132      0.916447   \n",
       "1  ag_news     TF-IDF+CNB      TF-IDF+SVM        0.898684      0.916447   \n",
       "2  ag_news  TF-IDF+LOGREG      TF-IDF+SVM        0.914868      0.916447   \n",
       "3  ag_news     TF-IDF+SVM      TF-IDF+SVM        0.916447      0.916447   \n",
       "4   amazon      BERT-base   TF-IDF+LOGREG        0.947850      0.876400   \n",
       "\n",
       "   delta_accuracy  model_energy_kwh  ref_energy_kwh  delta_energy_kwh  \\\n",
       "0        0.033684          0.058533        0.000240          0.058294   \n",
       "1       -0.017763          0.000038        0.000240         -0.000202   \n",
       "2       -0.001579          0.000196        0.000240         -0.000043   \n",
       "3        0.000000          0.000240        0.000240          0.000000   \n",
       "4        0.071450          0.199022        0.000186          0.198837   \n",
       "\n",
       "   joules_per_1pct_acc_vs_best_tfidf  \n",
       "0                       62301.278754  \n",
       "1                         409.042888  \n",
       "2                         986.004412  \n",
       "3                                NaN  \n",
       "4                      100183.577904  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview — energy normalization:\n"
     ]
    },
    {
     "data": {
      "application/vnd.deepnote.dataframe.v3+json": {
       "column_count": 4,
       "columns": [
        {
         "dtype": "object",
         "name": "dataset",
         "stats": {
          "categories": [
           {
            "count": 4,
            "name": "ag_news"
           },
           {
            "count": 1,
            "name": "amazon"
           }
          ],
          "histogram": null,
          "max": null,
          "min": null,
          "nan_count": 0,
          "unique_count": 2
         }
        },
        {
         "dtype": "object",
         "name": "model",
         "stats": {
          "categories": [
           {
            "count": 2,
            "name": "BERT-base"
           },
           {
            "count": 1,
            "name": "TF-IDF+CNB"
           },
           {
            "count": 2,
            "name": "2 others"
           }
          ],
          "histogram": null,
          "max": null,
          "min": null,
          "nan_count": 0,
          "unique_count": 4
         }
        },
        {
         "dtype": "float64",
         "name": "energy_kwh_per_test_example",
         "stats": {
          "categories": null,
          "histogram": [
           {
            "bin_end": 7.746498335002404e-7,
            "bin_start": 4.973194968689549e-9,
            "count": 3
           },
           {
            "bin_end": 0.0000015443264720317911,
            "bin_start": 7.746498335002404e-7,
            "count": 0
           },
           {
            "bin_end": 0.000002314003110563342,
            "bin_start": 0.0000015443264720317911,
            "count": 0
           },
           {
            "bin_end": 0.000003083679749094893,
            "bin_start": 0.000002314003110563342,
            "count": 0
           },
           {
            "bin_end": 0.000003853356387626444,
            "bin_start": 0.000003083679749094893,
            "count": 0
           },
           {
            "bin_end": 0.000004623033026157994,
            "bin_start": 0.000003853356387626444,
            "count": 0
           },
           {
            "bin_end": 0.000005392709664689545,
            "bin_start": 0.000004623033026157994,
            "count": 1
           },
           {
            "bin_end": 0.000006162386303221096,
            "bin_start": 0.000005392709664689545,
            "count": 0
           },
           {
            "bin_end": 0.0000069320629417526474,
            "bin_start": 0.000006162386303221096,
            "count": 0
           },
           {
            "bin_end": 0.000007701739580284198,
            "bin_start": 0.0000069320629417526474,
            "count": 1
           }
          ],
          "max": "7.701739580284198e-06",
          "min": "4.973194968689549e-09",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "energy_kwh_per_correct_prediction",
         "stats": {
          "categories": null,
          "histogram": [
           {
            "bin_end": 8.155776347959119e-7,
            "bin_start": 5.533862629874169e-9,
            "count": 3
           },
           {
            "bin_end": 0.0000016256214069619498,
            "bin_start": 8.155776347959119e-7,
            "count": 0
           },
           {
            "bin_end": 0.0000024356651791279873,
            "bin_start": 0.0000016256214069619498,
            "count": 0
           },
           {
            "bin_end": 0.000003245708951294025,
            "bin_start": 0.0000024356651791279873,
            "count": 0
           },
           {
            "bin_end": 0.000004055752723460063,
            "bin_start": 0.000003245708951294025,
            "count": 0
           },
           {
            "bin_end": 0.000004865796495626101,
            "bin_start": 0.000004055752723460063,
            "count": 0
           },
           {
            "bin_end": 0.000005675840267792139,
            "bin_start": 0.000004865796495626101,
            "count": 1
           },
           {
            "bin_end": 0.000006485884039958177,
            "bin_start": 0.000005675840267792139,
            "count": 0
           },
           {
            "bin_end": 0.000007295927812124214,
            "bin_start": 0.000006485884039958177,
            "count": 0
           },
           {
            "bin_end": 0.000008105971584290251,
            "bin_start": 0.000007295927812124214,
            "count": 1
           }
          ],
          "max": "8.105971584290251e-06",
          "min": "5.533862629874169e-09",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "int64",
         "name": "_deepnote_index_column"
        }
       ],
       "preview_row_count": 5,
       "row_count": 5,
       "rows": [
        {
         "_deepnote_index_column": 0,
         "dataset": "ag_news",
         "energy_kwh_per_correct_prediction": 0.000008105971584290251,
         "energy_kwh_per_test_example": 0.000007701739580284198,
         "model": "BERT-base"
        },
        {
         "_deepnote_index_column": 1,
         "dataset": "ag_news",
         "energy_kwh_per_correct_prediction": 5.533862629874169e-9,
         "energy_kwh_per_test_example": 4.973194968689549e-9,
         "model": "TF-IDF+CNB"
        },
        {
         "_deepnote_index_column": 2,
         "dataset": "ag_news",
         "energy_kwh_per_correct_prediction": 2.8244045223543796e-8,
         "energy_kwh_per_test_example": 2.5839585057802632e-8,
         "model": "TF-IDF+LOGREG"
        },
        {
         "_deepnote_index_column": 3,
         "dataset": "ag_news",
         "energy_kwh_per_correct_prediction": 3.44044011464178e-8,
         "energy_kwh_per_test_example": 3.152982289273684e-8,
         "model": "TF-IDF+SVM"
        },
        {
         "_deepnote_index_column": 4,
         "dataset": "amazon",
         "energy_kwh_per_correct_prediction": 0.000005249309693142029,
         "energy_kwh_per_test_example": 0.000004975558192644673,
         "model": "BERT-base"
        }
       ],
       "type": "dataframe"
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>energy_kwh_per_test_example</th>\n",
       "      <th>energy_kwh_per_correct_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ag_news</td>\n",
       "      <td>BERT-base</td>\n",
       "      <td>7.701740e-06</td>\n",
       "      <td>8.105972e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ag_news</td>\n",
       "      <td>TF-IDF+CNB</td>\n",
       "      <td>4.973195e-09</td>\n",
       "      <td>5.533863e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ag_news</td>\n",
       "      <td>TF-IDF+LOGREG</td>\n",
       "      <td>2.583959e-08</td>\n",
       "      <td>2.824405e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ag_news</td>\n",
       "      <td>TF-IDF+SVM</td>\n",
       "      <td>3.152982e-08</td>\n",
       "      <td>3.440440e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon</td>\n",
       "      <td>BERT-base</td>\n",
       "      <td>4.975558e-06</td>\n",
       "      <td>5.249310e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset          model  energy_kwh_per_test_example  \\\n",
       "0  ag_news      BERT-base                 7.701740e-06   \n",
       "1  ag_news     TF-IDF+CNB                 4.973195e-09   \n",
       "2  ag_news  TF-IDF+LOGREG                 2.583959e-08   \n",
       "3  ag_news     TF-IDF+SVM                 3.152982e-08   \n",
       "4   amazon      BERT-base                 4.975558e-06   \n",
       "\n",
       "   energy_kwh_per_correct_prediction  \n",
       "0                       8.105972e-06  \n",
       "1                       5.533863e-09  \n",
       "2                       2.824405e-08  \n",
       "3                       3.440440e-08  \n",
       "4                       5.249310e-06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CSV_PATH = \"model_comparison_report.csv\"\n",
    "results_df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "DATASETS = [d for d in [\"amazon\",\"ag_news\",\"dbpedia\"] if d in results_df[\"dataset\"].unique()]\n",
    "TEST_SIZE = {\"amazon\":40000, \"ag_news\":7600, \"dbpedia\":70000}\n",
    "J_PER_KWH = 3_600_000.0\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    try:\n",
    "        return np.nan if (b is None or float(b) == 0.0) else float(a)/float(b)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def best_tfidf_row(dsub: pd.DataFrame) -> pd.Series:\n",
    "    tf = dsub[dsub[\"model\"].str.contains(\"TF-IDF\", na=False)]\n",
    "    return tf.sort_values(\"accuracy\", ascending=False).iloc[0] if not tf.empty else pd.Series(dtype=float)\n",
    "\n",
    "# canonical, de-duplicated table of one row per (dataset, model)\n",
    "base = (\n",
    "    results_df\n",
    "    .loc[:, [\"dataset\",\"model\",\"accuracy\",\"energy_kwh\",\"emissions\"]]\n",
    "    .dropna(subset=[\"dataset\",\"model\"])\n",
    "    .drop_duplicates([\"dataset\",\"model\"], keep=\"last\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# per-dataset best TF-IDF + BERT rows and deltas\n",
    "deltas = []\n",
    "eff_rows = []\n",
    "per_example_rows = []\n",
    "for d in DATASETS:\n",
    "    sub = base[base[\"dataset\"] == d]\n",
    "    if sub.empty: \n",
    "        continue\n",
    "    ref = best_tfidf_row(sub)\n",
    "    if ref.empty:\n",
    "        continue\n",
    "    n_test = TEST_SIZE.get(d, np.nan)\n",
    "\n",
    "    # BERT vs best TF-IDF deltas\n",
    "    bert_row = sub[sub[\"model\"] == \"BERT-base\"]\n",
    "    if not bert_row.empty:\n",
    "        bert = bert_row.iloc[0]\n",
    "        delta_acc = float(bert[\"accuracy\"]) - float(ref[\"accuracy\"])\n",
    "        delta_energy_kwh = float(bert[\"energy_kwh\"]) - float(ref[\"energy_kwh\"])\n",
    "        delta_co2_kg = float(bert[\"emissions\"]) - float(ref[\"emissions\"])\n",
    "        j_per_1pct = _safe_div(delta_energy_kwh * J_PER_KWH, delta_acc * 100.0)\n",
    "\n",
    "        deltas.append({\n",
    "            \"dataset\": d,\n",
    "            \"ref_tfidf_model\": ref[\"model\"],\n",
    "            \"bert_accuracy\": float(bert[\"accuracy\"]),\n",
    "            \"ref_accuracy\": float(ref[\"accuracy\"]),\n",
    "            \"delta_accuracy\": delta_acc,\n",
    "            \"bert_energy_kwh\": float(bert[\"energy_kwh\"]),\n",
    "            \"ref_energy_kwh\": float(ref[\"energy_kwh\"]),\n",
    "            \"delta_energy_kwh\": delta_energy_kwh,\n",
    "            \"bert_co2_kg\": float(bert[\"emissions\"]),\n",
    "            \"ref_co2_kg\": float(ref[\"emissions\"]),\n",
    "            \"delta_co2_kg\": delta_co2_kg,\n",
    "            \"joules_per_1pct_acc_bert_vs_best_tfidf\": j_per_1pct,\n",
    "        })\n",
    "\n",
    "    # energy per example and per correct prediction for every model\n",
    "    for _, r in sub.iterrows():\n",
    "        acc = float(r[\"accuracy\"])\n",
    "        en = float(r[\"energy_kwh\"])\n",
    "        per_example_rows.append({\n",
    "            \"dataset\": d,\n",
    "            \"model\": r[\"model\"],\n",
    "            \"energy_kwh_per_test_example\": _safe_div(en, n_test),\n",
    "            \"energy_kwh_per_correct_prediction\": _safe_div(en, n_test * acc)\n",
    "        })\n",
    "\n",
    "    # J / +1% for every model vs best TF-IDF\n",
    "    for _, r in sub.iterrows():\n",
    "        acc = float(r[\"accuracy\"])\n",
    "        en = float(r[\"energy_kwh\"])\n",
    "        delta_acc_any = acc - float(ref[\"accuracy\"])\n",
    "        delta_en_any_kwh = en - float(ref[\"energy_kwh\"])\n",
    "        eff_rows.append({\n",
    "            \"dataset\": d,\n",
    "            \"model\": r[\"model\"],\n",
    "            \"ref_tfidf_model\": ref[\"model\"],\n",
    "            \"model_accuracy\": acc,\n",
    "            \"ref_accuracy\": float(ref[\"accuracy\"]),\n",
    "            \"delta_accuracy\": delta_acc_any,\n",
    "            \"model_energy_kwh\": en,\n",
    "            \"ref_energy_kwh\": float(ref[\"energy_kwh\"]),\n",
    "            \"delta_energy_kwh\": delta_en_any_kwh,\n",
    "            \"joules_per_1pct_acc_vs_best_tfidf\": _safe_div(delta_en_any_kwh * J_PER_KWH, delta_acc_any * 100.0)\n",
    "        })\n",
    "\n",
    "\n",
    "deltas_df = pd.DataFrame(deltas).sort_values(\"dataset\").reset_index(drop=True)\n",
    "eff_all_models_df = pd.DataFrame(eff_rows).sort_values([\"dataset\",\"model\"]).reset_index(drop=True)\n",
    "per_example_df = pd.DataFrame(per_example_rows).sort_values([\"dataset\",\"model\"]).reset_index(drop=True)\n",
    "\n",
    "best_tfidf_df = (\n",
    "    base[base[\"model\"].str.contains(\"TF-IDF\", na=False)]\n",
    "    .sort_values([\"dataset\",\"accuracy\"], ascending=[True, False])\n",
    "    .groupby(\"dataset\", as_index=False)\n",
    "    .first()\n",
    "    .rename(columns={\n",
    "        \"model\":\"best_tfidf_model\",\n",
    "        \"accuracy\":\"best_tfidf_accuracy\",\n",
    "        \"energy_kwh\":\"best_tfidf_energy_kwh\",\n",
    "        \"emissions\":\"best_tfidf_co2_kg\"\n",
    "    })\n",
    ")\n",
    "\n",
    "deltas_df.to_csv(\"metrics_deltas_bert_vs_best_tfidf.csv\", index=False)\n",
    "eff_all_models_df.to_csv(\"metrics_joules_per_1pct_all_models_vs_best_tfidf.csv\", index=False)\n",
    "per_example_df.to_csv(\"metrics_energy_normalized.csv\", index=False)\n",
    "best_tfidf_df.to_csv(\"metrics_best_tfidf_per_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "for fn in [\n",
    "    \"metrics_deltas_bert_vs_best_tfidf.csv\",\n",
    "    \"metrics_joules_per_1pct_all_models_vs_best_tfidf.csv\",\n",
    "    \"metrics_energy_normalized.csv\",\n",
    "    \"metrics_best_tfidf_per_dataset.csv\",\n",
    "]:\n",
    "    print(\" -\", fn)\n",
    "\n",
    "print(\"\\nPreview — deltas (BERT vs best TF-IDF):\")\n",
    "display(deltas_df.head())\n",
    "\n",
    "print(\"\\nPreview — J per +1% accuracy (all models vs best TF-IDF):\")\n",
    "display(eff_all_models_df.head())\n",
    "\n",
    "print(\"\\nPreview — energy normalization:\")\n",
    "display(per_example_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "59d8300cb58a4c53850c0d1d9f02435e",
  "deepnote_persisted_session": {
   "createdAt": "2025-09-09T12:25:48.781Z"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

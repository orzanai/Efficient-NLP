{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "e4a26018",
    "execution_start": 1757419031141,
    "execution_millis": 4986,
    "execution_context_id": "0b41266e-346f-44a4-8464-25a0535df017",
    "cell_id": "9ab56b27dc0342368ca182920d9a131a",
    "deepnote_cell_type": "code"
   },
   "source": "# Install/upgrade trainer deps\n!pip install -r requirements.txt",
   "block_group": "d9cfbdcfa01a4b21a2f64cb9223d3cff",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting accelerate>=0.30.0\n  Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m374.9/374.9 kB\u001B[0m \u001B[31m37.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting transformers>=4.40.0\n  Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.6/11.6 MB\u001B[0m \u001B[31m132.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting codecarbon\n  Downloading codecarbon-3.0.4-py3-none-any.whl (277 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m277.7/277.7 kB\u001B[0m \u001B[31m55.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting datasets\n  Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m494.8/494.8 kB\u001B[0m \u001B[31m86.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: scikit-learn in /root/venv/lib/python3.10/site-packages (1.1.3)\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.7/9.7 MB\u001B[0m \u001B[31m142.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: matplotlib in /root/venv/lib/python3.10/site-packages (3.6.3)\nCollecting matplotlib\n  Downloading matplotlib-3.10.6-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.7/8.7 MB\u001B[0m \u001B[31m176.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: seaborn in /root/venv/lib/python3.10/site-packages (0.13.2)\nCollecting huggingface_hub>=0.21.0\n  Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m561.5/561.5 kB\u001B[0m \u001B[31m79.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting torch>=2.0.0\n  Downloading torch-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl (888.0 MB)\n\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m574.9/888.0 MB\u001B[0m \u001B[31m258.9 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m^C\n\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m583.8/888.0 MB\u001B[0m \u001B[31m255.2 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[?25h\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
     "output_type": "stream"
    }
   ],
   "outputs_reference": "s3:deepnote-cell-outputs-production/3b9904b7-7d4d-4a68-b971-f7815c4069b8",
   "content_dependencies": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "26059997",
    "execution_start": 1757409913851,
    "execution_millis": 8573,
    "execution_context_id": "36ec1973-ad65-4964-8f93-960d5b34837a",
    "cell_id": "ca064b0e4cc9494ba6aaf04ca30fed07",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# Imports & helpers\n",
    "\n",
    "import os, random, numpy as np, pandas as pd, torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from codecarbon import EmissionsTracker\n",
    "import random as _rnd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "\n",
    "\n",
    "def get_energy_kwh(tracker) -> float:\n",
    "    data = getattr(tracker, \"final_emissions_data\", None) or getattr(tracker, \"_emissions_data\", None)\n",
    "    if data is not None:\n",
    "        energy = getattr(data, \"energy_consumed\", None)\n",
    "        if energy is not None:\n",
    "            return float(energy)\n",
    "        parts = [getattr(data, \"cpu_energy\", 0.0),\n",
    "                 getattr(data, \"gpu_energy\", 0.0),\n",
    "                 getattr(data, \"ram_energy\", 0.0)]\n",
    "        if any(parts):\n",
    "            return float(sum(p for p in parts if p is not None))\n",
    "    return 0.0\n",
    "\n",
    "def _stratified_cap_split(split_ds, label_field, max_n, seed=42):\n",
    "    if max_n is None or max_n >= len(split_ds):\n",
    "        return split_ds\n",
    "    labels = split_ds[label_field]\n",
    "    idx_by_label = {}\n",
    "    for i, y in enumerate(labels):\n",
    "        idx_by_label.setdefault(int(y), []).append(i)\n",
    "    rng = _rnd.Random(seed)\n",
    "    for y in idx_by_label: rng.shuffle(idx_by_label[y])\n",
    "    num_labels = len(idx_by_label)\n",
    "    base = max_n // num_labels\n",
    "    rem = max_n % num_labels\n",
    "    selected = []\n",
    "    for k, y in enumerate(sorted(idx_by_label.keys())):\n",
    "        take = min(base + (1 if k < rem else 0), len(idx_by_label[y]))\n",
    "        selected += idx_by_label[y][:take]\n",
    "    if len(selected) < max_n:\n",
    "        remaining = []\n",
    "        for k, y in enumerate(sorted(idx_by_label.keys())):\n",
    "            start = base + (1 if k < rem else 0)\n",
    "            remaining += idx_by_label[y][start:]\n",
    "        rng.shuffle(remaining)\n",
    "        need = max_n - len(selected)\n",
    "        selected += remaining[:need]\n",
    "    selected = selected[:max_n]\n",
    "    selected.sort()\n",
    "    return split_ds.select(selected)\n",
    "\n",
    "def cap_dataset_stratified(ds: DatasetDict, label_field: str, max_train=None, max_test=None, seed=42) -> DatasetDict:\n",
    "    out = {}\n",
    "    for split in ds.keys():\n",
    "        if split == \"train\":\n",
    "            out[split] = _stratified_cap_split(ds[split], label_field, max_train, seed)\n",
    "        elif split == \"test\":\n",
    "            out[split] = _stratified_cap_split(ds[split], label_field, max_test, seed)\n",
    "        else:\n",
    "            out[split] = ds[split]\n",
    "    return DatasetDict(out)\n"
   ],
   "block_group": "ca064b0e4cc9494ba6aaf04ca30fed07",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": "/root/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2025-09-09 09:25:19.942588: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-09-09 09:25:19.997314: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-09-09 09:25:19.997429: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-09-09 09:25:19.998583: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-09-09 09:25:20.006011: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-09-09 09:25:21.363928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
     "output_type": "stream"
    }
   ],
   "outputs_reference": "s3:deepnote-cell-outputs-production/297295f3-9022-4ce4-a49d-4f6c3d112d31",
   "content_dependencies": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "55615871",
    "execution_start": 1757409924902,
    "execution_millis": 12586,
    "execution_context_id": "36ec1973-ad65-4964-8f93-960d5b34837a",
    "cell_id": "402f96cdd3f944abb8d81ef255cfce1f",
    "deepnote_cell_type": "code"
   },
   "source": "# Load datasets (Amazon, AG News, DBpedia)\n\nraw_datasets = {\n    \"amazon\":  load_dataset(\"amazon_polarity\"),\n    \"ag_news\": load_dataset(\"ag_news\"),\n    \"dbpedia\": load_dataset(\"dbpedia_14\"),\n}\n\nDATA_META = {\n    \"amazon\":  {\"text\": \"content\", \"label\": \"label\", \"num_labels\": 2},\n    \"ag_news\": {\"text\": \"text\",    \"label\": \"label\", \"num_labels\": 4},\n    \"dbpedia\": {\"text\": \"content\", \"label\": \"label\", \"num_labels\": 14},\n}\n",
   "block_group": "f072011ae1ab4ebe8fc25b40ceb6d491",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": "Generating train split: 100%|██████████| 3600000/3600000 [00:05<00:00, 630483.46 examples/s]\nGenerating test split: 100%|██████████| 400000/400000 [00:00<00:00, 676839.88 examples/s]\nGenerating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1138686.42 examples/s]\nGenerating test split: 100%|██████████| 7600/7600 [00:00<00:00, 846052.24 examples/s]\nGenerating train split: 100%|██████████| 560000/560000 [00:00<00:00, 974044.22 examples/s] \nGenerating test split: 100%|██████████| 70000/70000 [00:00<00:00, 982966.50 examples/s]\n",
     "output_type": "stream"
    }
   ],
   "outputs_reference": "dbtable:cell_outputs/11c87d14-bf66-4625-b0c5-0eb19bdbc892",
   "content_dependencies": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "3e0bcdc2",
    "execution_start": 1756210837085,
    "execution_millis": 0,
    "execution_context_id": "68020ee9-7d3b-4fb5-ac0d-50d5c2e5a162",
    "cell_id": "15d1d1bfb733418a9e3e11b8c08eba64",
    "deepnote_cell_type": "code"
   },
   "source": "# TF-IDF baselines\n\ndef run_tfidf_experiment(name, dataset, text_field, label_field, model_type=\"logreg\"):\n    train_texts = dataset[\"train\"][text_field]\n    train_labels = dataset[\"train\"][label_field]\n    test_texts  = dataset[\"test\"][text_field]\n    test_labels = dataset[\"test\"][label_field]\n\n    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2), stop_words=\"english\")\n    X_train = vectorizer.fit_transform(train_texts)\n    X_test  = vectorizer.transform(test_texts)\n\n    tracker = EmissionsTracker(\n        project_name=f\"TFIDF_{model_type.upper()}_{name.upper()}\",\n        measure_power_secs=1, save_to_file=False, log_level=\"error\"\n    )\n    tracker.start()\n\n    if model_type == \"logreg\":\n        model = LogisticRegression(max_iter=2000, n_jobs=-1)\n    elif model_type == \"svm\":\n        model = LinearSVC()\n    elif model_type == \"cnb\":\n        model = ComplementNB()\n    else:\n        raise ValueError(\"model_type ∈ {'logreg','svm','cnb'}\")\n\n    model.fit(X_train, train_labels)\n    preds = model.predict(X_test)\n\n    emissions = tracker.stop()\n    energy_kwh = get_energy_kwh(tracker)\n    acc = accuracy_score(test_labels, preds)\n    print(f\"[TF-IDF + {model_type.upper()}] {name}  Acc: {acc:.4f}  CO₂: {emissions:.6f} kg  Energy: {energy_kwh:.6f} kWh\")\n\n    return {\n        \"model\": f\"TF-IDF+{model_type.upper()}\",\n        \"dataset\": name,\n        \"accuracy\": acc,\n        \"emissions\": emissions,\n        \"energy_kwh\": energy_kwh,\n    }\n",
   "block_group": "a50b7528c4ec4daf9d05794cc62b125a",
   "execution_count": 16,
   "outputs": [],
   "outputs_reference": null,
   "content_dependencies": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "6824a6b9",
    "execution_start": 1756210842500,
    "execution_millis": 3,
    "execution_context_id": "68020ee9-7d3b-4fb5-ac0d-50d5c2e5a162",
    "cell_id": "f2f61d916c414c5fafe58f04c97f024d",
    "deepnote_cell_type": "code"
   },
   "source": "# BERT-base runner (CUDA, AMP; tuned for T4)\n\ndef run_bert_experiment(name, dataset, text_field, label_field, num_labels, num_epochs=2):\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n    def tokenize_fn(examples):\n        return tokenizer(examples[text_field], padding=\"max_length\", truncation=True, max_length=128)\n\n    encoded = dataset.map(\n        tokenize_fn, batched=True,\n        remove_columns=[c for c in dataset[\"train\"].column_names if c not in {text_field, label_field}]\n    )\n    encoded = encoded.rename_column(label_field, \"labels\")\n    encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n    if torch.cuda.is_available():\n        model = model.to(\"cuda\")\n\n    args = TrainingArguments(\n        output_dir=f\"./results_{name}\",\n        eval_strategy=\"epoch\",  # transformers 4.55.x\n        per_device_train_batch_size=16,           \n        per_device_eval_batch_size=32,\n        num_train_epochs=num_epochs,              \n        gradient_accumulation_steps=1,\n        dataloader_num_workers=2,\n        dataloader_pin_memory=True,\n        save_total_limit=1,\n        report_to=\"none\",\n        logging_dir=f\"./logs_{name}\",\n        logging_strategy=\"steps\",\n        logging_steps=100,\n        fp16=torch.cuda.is_available(),           \n        no_cuda=False,\n        torch_compile=False,\n    )\n\n    def compute_metrics(eval_pred):\n        preds, labels = eval_pred\n        if isinstance(preds, tuple): preds = preds[0]\n        preds = np.argmax(preds, axis=-1)\n        return {\"accuracy\": accuracy_score(labels, preds)}\n\n    print(\"Trainer device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    trainer = Trainer(\n        model=model, args=args,\n        train_dataset=encoded[\"train\"],\n        eval_dataset=encoded[\"test\"],\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n\n    tracker = EmissionsTracker(\n        project_name=f\"BERT_{name.upper()}\",\n        measure_power_secs=1, save_to_file=False, log_level=\"error\"\n    )\n    tracker.start()\n    trainer.train()\n    emissions = tracker.stop()\n    energy_kwh = get_energy_kwh(tracker)\n\n    acc = trainer.evaluate()[\"eval_accuracy\"]\n    print(f\"[BERT] {name}  Acc: {acc:.4f}  CO₂: {emissions:.6f} kg  Energy: {energy_kwh:.6f} kWh\")\n\n    return {\n        \"model\": \"BERT-base\",\n        \"dataset\": name,\n        \"accuracy\": acc,\n        \"emissions\": emissions,\n        \"energy_kwh\": energy_kwh,\n    }\n",
   "block_group": "038df9d73a6d468eb330febbd1b45252",
   "execution_count": 19,
   "outputs": [],
   "outputs_reference": null,
   "content_dependencies": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "1da1f1",
    "execution_start": 1756210853509,
    "execution_millis": 26161937,
    "execution_context_id": "68020ee9-7d3b-4fb5-ac0d-50d5c2e5a162",
    "cell_id": "938c5ca96442499b8bbae4b716d0fca7",
    "deepnote_cell_type": "code"
   },
   "source": "# Comprehensive experiment\n\nAMAZON_CAP_TRAIN = 400_000  \nAMAZON_CAP_TEST  = 40_000\n\nresults = []\nmodel_types = [\"logreg\", \"svm\", \"cnb\"]\n\nfor dataset_key, meta in {\n    \"amazon\":  {\"text\": \"content\", \"label\": \"label\", \"num_labels\": 2},\n    \"ag_news\": {\"text\": \"text\",    \"label\": \"label\", \"num_labels\": 4},\n    \"dbpedia\": {\"text\": \"content\", \"label\": \"label\", \"num_labels\": 14},\n}.items():\n    if dataset_key == \"amazon\":\n        data = cap_dataset_stratified(raw_datasets[\"amazon\"], meta[\"label\"],\n                                      max_train=AMAZON_CAP_TRAIN, max_test=AMAZON_CAP_TEST, seed=SEED)\n    else:\n        data = raw_datasets[dataset_key]\n\n    # TF-IDF trio\n    for m in model_types:\n        results.append(run_tfidf_experiment(dataset_key, data, meta[\"text\"], meta[\"label\"], model_type=m))\n\n    # BERT-base (CUDA+AMP) \n    results.append(run_bert_experiment(dataset_key, data, meta[\"text\"], meta[\"label\"], meta[\"num_labels\"], num_epochs=2))\n\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"model_comparison_report.csv\", index=False)\nresults_df\n",
   "block_group": "829db54e6191475cb0b551371511f505",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "text": "[codecarbon WARNING @ 08:23:26] Multiple instances of codecarbon are allowed to run at the same time.\n[TF-IDF + LOGREG] amazon  Acc: 0.8764  CO₂: 0.000069 kg  Energy: 0.000186 kWh\n[TF-IDF + SVM] amazon  Acc: 0.8763  CO₂: 0.000170 kg  Energy: 0.000460 kWh\n[TF-IDF + CNB] amazon  Acc: 0.8380  CO₂: 0.000064 kg  Energy: 0.000174 kWh\nMap: 100%|██████████| 400000/400000 [01:11<00:00, 5581.22 examples/s]\nMap: 100%|██████████| 40000/40000 [00:07<00:00, 5670.57 examples/s]\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTrainer device: cuda\n/tmp/ipykernel_75/549541287.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/50000 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   1/1250 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "[BERT] amazon  Acc: 0.9478  CO₂: 0.073466 kg  Energy: 0.199022 kWh\n[TF-IDF + LOGREG] ag_news  Acc: 0.9149  CO₂: 0.000072 kg  Energy: 0.000196 kWh\n[TF-IDF + SVM] ag_news  Acc: 0.9164  CO₂: 0.000088 kg  Energy: 0.000240 kWh\n[TF-IDF + CNB] ag_news  Acc: 0.8987  CO₂: 0.000014 kg  Energy: 0.000038 kWh\nMap: 100%|██████████| 120000/120000 [00:15<00:00, 7882.69 examples/s]\nMap: 100%|██████████| 7600/7600 [00:00<00:00, 8636.44 examples/s]\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_75/549541287.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nTrainer device: cuda\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/15000 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/238 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "[BERT] ag_news  Acc: 0.9501  CO₂: 0.021607 kg  Energy: 0.058533 kWh\n[TF-IDF + LOGREG] dbpedia  Acc: 0.9779  CO₂: 0.000294 kg  Energy: 0.000797 kWh\n[TF-IDF + SVM] dbpedia  Acc: 0.9783  CO₂: 0.000483 kg  Energy: 0.001308 kWh\n[TF-IDF + CNB] dbpedia  Acc: 0.9377  CO₂: 0.000047 kg  Energy: 0.000126 kWh\nMap: 100%|██████████| 560000/560000 [01:13<00:00, 7648.26 examples/s]\nMap: 100%|██████████| 70000/70000 [00:09<00:00, 7680.24 examples/s]\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_75/549541287.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nTrainer device: cuda\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='70000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/70000 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='2188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   1/2188 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "[BERT] dbpedia  Acc: 0.9923  CO₂: 0.102488 kg  Energy: 0.277645 kWh\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 22,
     "data": {
      "application/vnd.deepnote.dataframe.v3+json": {
       "column_count": 5,
       "columns": [
        {
         "name": "model",
         "dtype": "object",
         "stats": {
          "unique_count": 4,
          "nan_count": 0,
          "min": null,
          "max": null,
          "histogram": null,
          "categories": [
           {
            "name": "TF-IDF+LOGREG",
            "count": 3
           },
           {
            "name": "TF-IDF+SVM",
            "count": 3
           },
           {
            "name": "2 others",
            "count": 6
           }
          ]
         }
        },
        {
         "name": "dataset",
         "dtype": "object",
         "stats": {
          "unique_count": 3,
          "nan_count": 0,
          "min": null,
          "max": null,
          "histogram": null,
          "categories": [
           {
            "name": "amazon",
            "count": 4
           },
           {
            "name": "ag_news",
            "count": 4
           },
           {
            "name": "dbpedia",
            "count": 4
           }
          ]
         }
        },
        {
         "name": "accuracy",
         "dtype": "float64",
         "stats": {
          "unique_count": 12,
          "nan_count": 0,
          "min": "0.83805",
          "max": "0.9922714285714286",
          "histogram": [
           {
            "bin_start": 0.83805,
            "bin_end": 0.8534721428571428,
            "count": 1
           },
           {
            "bin_start": 0.8534721428571428,
            "bin_end": 0.8688942857142857,
            "count": 0
           },
           {
            "bin_start": 0.8688942857142857,
            "bin_end": 0.8843164285714286,
            "count": 2
           },
           {
            "bin_start": 0.8843164285714286,
            "bin_end": 0.8997385714285714,
            "count": 1
           },
           {
            "bin_start": 0.8997385714285714,
            "bin_end": 0.9151607142857143,
            "count": 1
           },
           {
            "bin_start": 0.9151607142857143,
            "bin_end": 0.9305828571428572,
            "count": 1
           },
           {
            "bin_start": 0.9305828571428572,
            "bin_end": 0.946005,
            "count": 1
           },
           {
            "bin_start": 0.946005,
            "bin_end": 0.9614271428571428,
            "count": 2
           },
           {
            "bin_start": 0.9614271428571428,
            "bin_end": 0.9768492857142858,
            "count": 0
           },
           {
            "bin_start": 0.9768492857142858,
            "bin_end": 0.9922714285714286,
            "count": 3
           }
          ],
          "categories": null
         }
        },
        {
         "name": "emissions",
         "dtype": "float64",
         "stats": {
          "unique_count": 12,
          "nan_count": 0,
          "min": "1.3951906954813969e-05",
          "max": "0.102488335441725",
          "histogram": [
           {
            "bin_start": 1.3951906954813969E-5,
            "bin_end": 0.010261390260431833,
            "count": 9
           },
           {
            "bin_start": 0.010261390260431833,
            "bin_end": 0.020508828613908854,
            "count": 0
           },
           {
            "bin_start": 0.020508828613908854,
            "bin_end": 0.030756266967385874,
            "count": 1
           },
           {
            "bin_start": 0.030756266967385874,
            "bin_end": 0.04100370532086289,
            "count": 0
           },
           {
            "bin_start": 0.04100370532086289,
            "bin_end": 0.051251143674339916,
            "count": 0
           },
           {
            "bin_start": 0.051251143674339916,
            "bin_end": 0.06149858202781693,
            "count": 0
           },
           {
            "bin_start": 0.06149858202781693,
            "bin_end": 0.07174602038129395,
            "count": 0
           },
           {
            "bin_start": 0.07174602038129395,
            "bin_end": 0.08199345873477097,
            "count": 1
           },
           {
            "bin_start": 0.08199345873477097,
            "bin_end": 0.092240897088248,
            "count": 0
           },
           {
            "bin_start": 0.092240897088248,
            "bin_end": 0.102488335441725,
            "count": 1
           }
          ],
          "categories": null
         }
        },
        {
         "name": "energy_kwh",
         "dtype": "float64",
         "stats": {
          "unique_count": 12,
          "nan_count": 0,
          "min": "3.7796281762040574e-05",
          "max": "0.277645057139762",
          "histogram": [
           {
            "bin_start": 3.7796281762040574E-5,
            "bin_end": 0.02779852236756204,
            "count": 9
           },
           {
            "bin_start": 0.02779852236756204,
            "bin_end": 0.05555924845336204,
            "count": 0
           },
           {
            "bin_start": 0.05555924845336204,
            "bin_end": 0.08331997453916204,
            "count": 1
           },
           {
            "bin_start": 0.08331997453916204,
            "bin_end": 0.11108070062496203,
            "count": 0
           },
           {
            "bin_start": 0.11108070062496203,
            "bin_end": 0.13884142671076202,
            "count": 0
           },
           {
            "bin_start": 0.13884142671076202,
            "bin_end": 0.16660215279656204,
            "count": 0
           },
           {
            "bin_start": 0.16660215279656204,
            "bin_end": 0.19436287888236203,
            "count": 0
           },
           {
            "bin_start": 0.19436287888236203,
            "bin_end": 0.22212360496816203,
            "count": 1
           },
           {
            "bin_start": 0.22212360496816203,
            "bin_end": 0.24988433105396202,
            "count": 0
           },
           {
            "bin_start": 0.24988433105396202,
            "bin_end": 0.277645057139762,
            "count": 1
           }
          ],
          "categories": null
         }
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "row_count": 12,
       "preview_row_count": 12,
       "rows": [
        {
         "model": "TF-IDF+LOGREG",
         "dataset": "amazon",
         "accuracy": 0.8764,
         "emissions": 6.856831167350798E-5,
         "energy_kwh": 1.8575433712056887E-4,
         "_deepnote_index_column": 0
        },
        {
         "model": "TF-IDF+SVM",
         "dataset": "amazon",
         "accuracy": 0.8763,
         "emissions": 1.6974310189318606E-4,
         "energy_kwh": 4.598409469828039E-4,
         "_deepnote_index_column": 1
        },
        {
         "model": "TF-IDF+CNB",
         "dataset": "amazon",
         "accuracy": 0.83805,
         "emissions": 6.432596228426539E-5,
         "energy_kwh": 1.7426164057606473E-4,
         "_deepnote_index_column": 2
        },
        {
         "model": "BERT-base",
         "dataset": "amazon",
         "accuracy": 0.94785,
         "emissions": 0.07346598312404264,
         "energy_kwh": 0.1990223277057869,
         "_deepnote_index_column": 3
        },
        {
         "model": "TF-IDF+LOGREG",
         "dataset": "ag_news",
         "accuracy": 0.9148684210526316,
         "emissions": 7.249092158004209E-5,
         "energy_kwh": 1.963808464393383E-4,
         "_deepnote_index_column": 4
        },
        {
         "model": "TF-IDF+SVM",
         "dataset": "ag_news",
         "accuracy": 0.9164473684210527,
         "emissions": 8.845443584472989E-5,
         "energy_kwh": 2.3962665398483016E-4,
         "_deepnote_index_column": 5
        },
        {
         "model": "TF-IDF+CNB",
         "dataset": "ag_news",
         "accuracy": 0.8986842105263158,
         "emissions": 1.3951906954813969E-5,
         "energy_kwh": 3.7796281762040574E-5,
         "_deepnote_index_column": 6
        },
        {
         "model": "BERT-base",
         "dataset": "ag_news",
         "accuracy": 0.9501315789473684,
         "emissions": 0.021606624049699725,
         "energy_kwh": 0.05853322081015999,
         "_deepnote_index_column": 7
        },
        {
         "model": "TF-IDF+LOGREG",
         "dataset": "dbpedia",
         "accuracy": 0.9779285714285715,
         "emissions": 2.9402929341036156E-4,
         "energy_kwh": 7.96537280245925E-4,
         "_deepnote_index_column": 8
        },
        {
         "model": "TF-IDF+SVM",
         "dataset": "dbpedia",
         "accuracy": 0.9783142857142857,
         "emissions": 4.827877213638346E-4,
         "energy_kwh": 0.0013078915166951338,
         "_deepnote_index_column": 9
        }
       ],
       "type": "dataframe"
      },
      "text/plain": "            model  dataset  accuracy  emissions  energy_kwh\n0   TF-IDF+LOGREG   amazon  0.876400   0.000069    0.000186\n1      TF-IDF+SVM   amazon  0.876300   0.000170    0.000460\n2      TF-IDF+CNB   amazon  0.838050   0.000064    0.000174\n3       BERT-base   amazon  0.947850   0.073466    0.199022\n4   TF-IDF+LOGREG  ag_news  0.914868   0.000072    0.000196\n5      TF-IDF+SVM  ag_news  0.916447   0.000088    0.000240\n6      TF-IDF+CNB  ag_news  0.898684   0.000014    0.000038\n7       BERT-base  ag_news  0.950132   0.021607    0.058533\n8   TF-IDF+LOGREG  dbpedia  0.977929   0.000294    0.000797\n9      TF-IDF+SVM  dbpedia  0.978314   0.000483    0.001308\n10     TF-IDF+CNB  dbpedia  0.937686   0.000047    0.000126\n11      BERT-base  dbpedia  0.992271   0.102488    0.277645",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>dataset</th>\n      <th>accuracy</th>\n      <th>emissions</th>\n      <th>energy_kwh</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TF-IDF+LOGREG</td>\n      <td>amazon</td>\n      <td>0.876400</td>\n      <td>0.000069</td>\n      <td>0.000186</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TF-IDF+SVM</td>\n      <td>amazon</td>\n      <td>0.876300</td>\n      <td>0.000170</td>\n      <td>0.000460</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TF-IDF+CNB</td>\n      <td>amazon</td>\n      <td>0.838050</td>\n      <td>0.000064</td>\n      <td>0.000174</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>BERT-base</td>\n      <td>amazon</td>\n      <td>0.947850</td>\n      <td>0.073466</td>\n      <td>0.199022</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TF-IDF+LOGREG</td>\n      <td>ag_news</td>\n      <td>0.914868</td>\n      <td>0.000072</td>\n      <td>0.000196</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>TF-IDF+SVM</td>\n      <td>ag_news</td>\n      <td>0.916447</td>\n      <td>0.000088</td>\n      <td>0.000240</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>TF-IDF+CNB</td>\n      <td>ag_news</td>\n      <td>0.898684</td>\n      <td>0.000014</td>\n      <td>0.000038</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>BERT-base</td>\n      <td>ag_news</td>\n      <td>0.950132</td>\n      <td>0.021607</td>\n      <td>0.058533</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>TF-IDF+LOGREG</td>\n      <td>dbpedia</td>\n      <td>0.977929</td>\n      <td>0.000294</td>\n      <td>0.000797</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>TF-IDF+SVM</td>\n      <td>dbpedia</td>\n      <td>0.978314</td>\n      <td>0.000483</td>\n      <td>0.001308</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>TF-IDF+CNB</td>\n      <td>dbpedia</td>\n      <td>0.937686</td>\n      <td>0.000047</td>\n      <td>0.000126</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>BERT-base</td>\n      <td>dbpedia</td>\n      <td>0.992271</td>\n      <td>0.102488</td>\n      <td>0.277645</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "outputs_reference": "s3:deepnote-cell-outputs-production/a8fea9c4-a651-46f9-abf5-2f9e5758d812",
   "content_dependencies": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "82dc23fb",
    "execution_start": 1757418764891,
    "execution_millis": 4102,
    "execution_context_id": "0b41266e-346f-44a4-8464-25a0535df017",
    "cell_id": "99e83fe3dc8842caaa049a6ad2db9bc2",
    "deepnote_cell_type": "code"
   },
   "source": "# Visualizations\n\ndf = pd.read_csv(\"model_comparison_report.csv\")\n\nmodel_order = [\"TF-IDF+LOGREG\", \"TF-IDF+SVM\", \"TF-IDF+CNB\", \"BERT-base\"]\ncolors = {\"TF-IDF+LOGREG\":\"#1f77b4\",\"TF-IDF+SVM\":\"#ff7f0e\",\"TF-IDF+CNB\":\"#2ca02c\",\"BERT-base\":\"#d62728\"}\nmarkers = {\"TF-IDF+LOGREG\":\"o\",\"TF-IDF+SVM\":\"s\",\"TF-IDF+CNB\":\"^\",\"BERT-base\":\"X\"}\n\nplt.figure(figsize=(10,6))\ndf_sorted = df.set_index(\"model\").loc[model_order].reset_index()\nfor dataset in df_sorted[\"dataset\"].unique():\n    sub = df_sorted[df_sorted[\"dataset\"]==dataset]\n    plt.bar(sub[\"model\"], sub[\"accuracy\"], color=[colors[m] for m in sub[\"model\"]])\nplt.title(\"Accuracy by Model and Dataset\")\nplt.ylabel(\"Accuracy\")\nplt.legend(model_order, bbox_to_anchor=(1.05,1), loc=\"upper left\")\nplt.tight_layout()\nplt.savefig(\"accuracy_by_model_grouped.png\", dpi=300)\nplt.close()\n\ndatasets = df[\"dataset\"].unique()\nfig, axes = plt.subplots(1, len(datasets), figsize=(18,6), sharey=True)\nfor ax,dataset in zip(axes,datasets):\n    sub = df[df[\"dataset\"]==dataset]\n    for m in model_order:\n        row = sub[sub[\"model\"]==m]\n        if not row.empty:\n            ax.scatter(row[\"accuracy\"], row[\"energy_kwh\"], color=colors[m], marker=markers[m], s=100, label=m)\n            ax.text(row[\"accuracy\"].values[0], row[\"energy_kwh\"].values[0], m.split(\"+\")[-1], fontsize=8, ha=\"center\", va=\"bottom\")\n    ax.set_title(dataset)\n    ax.set_xlabel(\"Accuracy\")\n    ax.set_yscale(\"log\")\naxes[0].set_ylabel(\"Energy (kWh, log scale)\")\nhandles = [plt.Line2D([0],[0], marker=markers[m], color='w', markerfacecolor=colors[m], markersize=10, label=m) for m in model_order]\nfig.legend(handles=handles, labels=model_order, loc=\"upper center\", ncol=4)\nfig.suptitle(\"Accuracy vs Energy (kWh, log scale) — Models per Dataset\", y=1.02)\nplt.tight_layout()\nplt.savefig(\"accuracy_vs_energy_by_model_faceted_log.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\n\nplt.figure(figsize=(10,6))\ndf_sorted = df.set_index(\"model\").loc[model_order].reset_index()\nfor dataset in df_sorted[\"dataset\"].unique():\n    sub = df_sorted[df_sorted[\"dataset\"]==dataset]\n    plt.bar(sub[\"model\"], sub[\"energy_kwh\"], color=[colors[m] for m in sub[\"model\"]])\nplt.yscale(\"log\")\nplt.title(\"Energy (kWh) by Model and Dataset\")\nplt.ylabel(\"Energy (kWh, log scale)\")\nplt.legend(model_order, bbox_to_anchor=(1.05,1), loc=\"upper left\")\nplt.tight_layout()\nplt.savefig(\"energy_by_model_grouped.png\", dpi=300)\nplt.close()\n\nplt.figure(figsize=(12,6))\nfor dataset in df[\"dataset\"].unique():\n    sub = df[df[\"dataset\"]==dataset]\n    n_test = sub[\"accuracy\"].values[0]*10000\n    sub[\"energy_per_correct\"] = sub[\"energy_kwh\"]/(sub[\"accuracy\"]*n_test)\n    plt.bar([f\"{dataset}-{m}\" for m in sub[\"model\"]], sub[\"energy_per_correct\"], color=[colors[m] for m in sub[\"model\"]])\nplt.yscale(\"log\")\nplt.title(\"Energy per Correct Prediction by Model within Dataset\")\nplt.ylabel(\"kWh per Correct Prediction (log scale)\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.tight_layout()\nplt.savefig(\"F3_kwh_per_correct.png\", dpi=300)\nplt.close()\n\nrows = []\nfor dataset in df[\"dataset\"].unique():\n    sub = df[df[\"dataset\"]==dataset]\n    tfidf_best = sub[sub[\"model\"].str.contains(\"TF-IDF\")].sort_values(\"accuracy\", ascending=False).iloc[0]\n    bert = sub[sub[\"model\"]==\"BERT-base\"].iloc[0]\n    rows.append({\n        \"dataset\":dataset,\n        \"delta_acc\": bert[\"accuracy\"]-tfidf_best[\"accuracy\"],\n        \"delta_energy\": bert[\"energy_kwh\"]-tfidf_best[\"energy_kwh\"]\n    })\ncomp = pd.DataFrame(rows)\n\nfig, axes = plt.subplots(1,2,figsize=(12,6),sharey=True)\naxes[0].barh(comp[\"dataset\"], comp[\"delta_acc\"])\naxes[0].set_xlabel(\"ΔAccuracy (BERT – best TF-IDF)\")\naxes[0].set_title(\"Accuracy Gain of BERT vs best TF-IDF\")\naxes[1].barh(comp[\"dataset\"], comp[\"delta_energy\"])\naxes[1].set_xlabel(\"ΔEnergy (kWh) (BERT – best TF-IDF)\")\naxes[1].set_title(\"Energy Overhead of BERT vs best TF-IDF\")\nfig.suptitle(\"BERT vs best TF-IDF — Accuracy Gain vs Energy Overhead\")\nplt.tight_layout()\nplt.savefig(\"F5_tfidf_vs_bert_deltas.png\", dpi=300)\nplt.close()\n",
   "block_group": "cb39f4e7b3fc45718b09759cf48befa9",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_83/3036000435.py:60: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sub[\"energy_per_correct\"] = sub[\"energy_kwh\"]/(sub[\"accuracy\"]*n_test)\n/tmp/ipykernel_83/3036000435.py:60: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sub[\"energy_per_correct\"] = sub[\"energy_kwh\"]/(sub[\"accuracy\"]*n_test)\n/tmp/ipykernel_83/3036000435.py:60: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sub[\"energy_per_correct\"] = sub[\"energy_kwh\"]/(sub[\"accuracy\"]*n_test)\n",
     "output_type": "stream"
    }
   ],
   "outputs_reference": "s3:deepnote-cell-outputs-production/cd2010d2-853b-4e3a-bfa8-2ec5ee2ce180",
   "content_dependencies": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Efficiency table (BERT vs best TF-IDF per dataset)\n",
    "\n",
    "J_PER_KWH = 3_600_000.0\n",
    "\n",
    "def _safe_div(a, b, default=np.nan):\n",
    "    try:\n",
    "        return a / b if b != 0 else default\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "TEST_SIZES = {\n",
    "    \"amazon\": 40000,\n",
    "    \"ag_news\": len(raw_datasets[\"ag_news\"][\"test\"]),\n",
    "    \"dbpedia\": len(raw_datasets[\"dbpedia\"][\"test\"]),\n",
    "}\n",
    "\n",
    "def compute_efficiency(df):\n",
    "    rows = []\n",
    "    for dataset in df[\"dataset\"].unique():\n",
    "        sub = df[df[\"dataset\"] == dataset]\n",
    "        tfidf_best = sub[sub[\"model\"].str.contains(\"TF-IDF\")].sort_values(\"accuracy\", ascending=False).iloc[0]\n",
    "        bert = sub[sub[\"model\"] == \"BERT-base\"].iloc[0]\n",
    "\n",
    "        n_test = TEST_SIZES.get(dataset, np.nan)\n",
    "        delta_acc = float(bert[\"accuracy\"] - tfidf_best[\"accuracy\"])\n",
    "        delta_energy_kwh = float(bert[\"energy_kwh\"] - tfidf_best[\"energy_kwh\"])\n",
    "        joules_per_1pct = _safe_div(delta_energy_kwh * J_PER_KWH, (delta_acc * 100.0), default=np.inf)\n",
    "\n",
    "        rows.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"best_tfidf_model\": tfidf_best[\"model\"],\n",
    "            \"tfidf_accuracy\": float(tfidf_best[\"accuracy\"]),\n",
    "            \"bert_accuracy\": float(bert[\"accuracy\"]),\n",
    "            \"delta_accuracy\": delta_acc,\n",
    "            \"tfidf_energy_kwh\": float(tfidf_best[\"energy_kwh\"]),\n",
    "            \"bert_energy_kwh\": float(bert[\"energy_kwh\"]),\n",
    "            \"delta_energy_kwh\": delta_energy_kwh,\n",
    "            \"tfidf_co2_kg\": float(tfidf_best[\"emissions\"]),\n",
    "            \"bert_co2_kg\": float(bert[\"emissions\"]),\n",
    "            \"tfidf_energy_per_test_ex_kWh\": _safe_div(float(tfidf_best[\"energy_kwh\"]), n_test),\n",
    "            \"bert_energy_per_test_ex_kWh\": _safe_div(float(bert[\"energy_kwh\"]), n_test),\n",
    "            \"joules_per_1pct_acc\": joules_per_1pct,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "efficiency_df = compute_efficiency(results_df)\n",
    "efficiency_df.to_csv(\"energy_efficiency_report.csv\", index=False)\n",
    "with open(\"energy_efficiency_report.tex\",\"w\") as f:\n",
    "    f.write(efficiency_df.to_latex(index=False, float_format=\"%.6g\"))\n",
    "efficiency_df"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "37006ca3",
    "execution_start": 1757419647013,
    "execution_millis": 771,
    "execution_context_id": "c3e9c748-36fe-401f-bb64-5c507344e556",
    "cell_id": "0f203196697a4be698eaf32b04d84831",
    "deepnote_cell_type": "code"
   },
   "source": [
    "CSV_PATH = \"model_comparison_report.csv\"\n",
    "results_df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "DATASETS = [d for d in [\"amazon\",\"ag_news\",\"dbpedia\"] if d in results_df[\"dataset\"].unique()]\n",
    "TEST_SIZE = {\"amazon\":40000, \"ag_news\":7600, \"dbpedia\":70000}\n",
    "J_PER_KWH = 3_600_000.0\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    try:\n",
    "        return np.nan if (b is None or float(b) == 0.0) else float(a)/float(b)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def best_tfidf_row(dsub: pd.DataFrame) -> pd.Series:\n",
    "    tf = dsub[dsub[\"model\"].str.contains(\"TF-IDF\", na=False)]\n",
    "    return tf.sort_values(\"accuracy\", ascending=False).iloc[0] if not tf.empty else pd.Series(dtype=float)\n",
    "\n",
    "# canonical, de-duplicated table of one row per (dataset, model)\n",
    "base = (\n",
    "    results_df\n",
    "    .loc[:, [\"dataset\",\"model\",\"accuracy\",\"energy_kwh\",\"emissions\"]]\n",
    "    .dropna(subset=[\"dataset\",\"model\"])\n",
    "    .drop_duplicates([\"dataset\",\"model\"], keep=\"last\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# per-dataset best TF-IDF + BERT rows and deltas\n",
    "deltas = []\n",
    "eff_rows = []\n",
    "per_example_rows = []\n",
    "for d in DATASETS:\n",
    "    sub = base[base[\"dataset\"] == d]\n",
    "    if sub.empty: \n",
    "        continue\n",
    "    ref = best_tfidf_row(sub)\n",
    "    if ref.empty:\n",
    "        continue\n",
    "    n_test = TEST_SIZE.get(d, np.nan)\n",
    "\n",
    "    # BERT vs best TF-IDF deltas\n",
    "    bert_row = sub[sub[\"model\"] == \"BERT-base\"]\n",
    "    if not bert_row.empty:\n",
    "        bert = bert_row.iloc[0]\n",
    "        delta_acc = float(bert[\"accuracy\"]) - float(ref[\"accuracy\"])\n",
    "        delta_energy_kwh = float(bert[\"energy_kwh\"]) - float(ref[\"energy_kwh\"])\n",
    "        delta_co2_kg = float(bert[\"emissions\"]) - float(ref[\"emissions\"])\n",
    "        j_per_1pct = _safe_div(delta_energy_kwh * J_PER_KWH, delta_acc * 100.0)\n",
    "\n",
    "        deltas.append({\n",
    "            \"dataset\": d,\n",
    "            \"ref_tfidf_model\": ref[\"model\"],\n",
    "            \"bert_accuracy\": float(bert[\"accuracy\"]),\n",
    "            \"ref_accuracy\": float(ref[\"accuracy\"]),\n",
    "            \"delta_accuracy\": delta_acc,\n",
    "            \"bert_energy_kwh\": float(bert[\"energy_kwh\"]),\n",
    "            \"ref_energy_kwh\": float(ref[\"energy_kwh\"]),\n",
    "            \"delta_energy_kwh\": delta_energy_kwh,\n",
    "            \"bert_co2_kg\": float(bert[\"emissions\"]),\n",
    "            \"ref_co2_kg\": float(ref[\"emissions\"]),\n",
    "            \"delta_co2_kg\": delta_co2_kg,\n",
    "            \"joules_per_1pct_acc_bert_vs_best_tfidf\": j_per_1pct,\n",
    "        })\n",
    "\n",
    "    # energy per example and per correct prediction for every model\n",
    "    for _, r in sub.iterrows():\n",
    "        acc = float(r[\"accuracy\"])\n",
    "        en = float(r[\"energy_kwh\"])\n",
    "        per_example_rows.append({\n",
    "            \"dataset\": d,\n",
    "            \"model\": r[\"model\"],\n",
    "            \"energy_kwh_per_test_example\": _safe_div(en, n_test),\n",
    "            \"energy_kwh_per_correct_prediction\": _safe_div(en, n_test * acc)\n",
    "        })\n",
    "\n",
    "    # J / +1% for every model vs best TF-IDF\n",
    "    for _, r in sub.iterrows():\n",
    "        acc = float(r[\"accuracy\"])\n",
    "        en = float(r[\"energy_kwh\"])\n",
    "        delta_acc_any = acc - float(ref[\"accuracy\"])\n",
    "        delta_en_any_kwh = en - float(ref[\"energy_kwh\"])\n",
    "        eff_rows.append({\n",
    "            \"dataset\": d,\n",
    "            \"model\": r[\"model\"],\n",
    "            \"ref_tfidf_model\": ref[\"model\"],\n",
    "            \"model_accuracy\": acc,\n",
    "            \"ref_accuracy\": float(ref[\"accuracy\"]),\n",
    "            \"delta_accuracy\": delta_acc_any,\n",
    "            \"model_energy_kwh\": en,\n",
    "            \"ref_energy_kwh\": float(ref[\"energy_kwh\"]),\n",
    "            \"delta_energy_kwh\": delta_en_any_kwh,\n",
    "            \"joules_per_1pct_acc_vs_best_tfidf\": _safe_div(delta_en_any_kwh * J_PER_KWH, delta_acc_any * 100.0)\n",
    "        })\n",
    "\n",
    "\n",
    "deltas_df = pd.DataFrame(deltas).sort_values(\"dataset\").reset_index(drop=True)\n",
    "eff_all_models_df = pd.DataFrame(eff_rows).sort_values([\"dataset\",\"model\"]).reset_index(drop=True)\n",
    "per_example_df = pd.DataFrame(per_example_rows).sort_values([\"dataset\",\"model\"]).reset_index(drop=True)\n",
    "\n",
    "best_tfidf_df = (\n",
    "    base[base[\"model\"].str.contains(\"TF-IDF\", na=False)]\n",
    "    .sort_values([\"dataset\",\"accuracy\"], ascending=[True, False])\n",
    "    .groupby(\"dataset\", as_index=False)\n",
    "    .first()\n",
    "    .rename(columns={\n",
    "        \"model\":\"best_tfidf_model\",\n",
    "        \"accuracy\":\"best_tfidf_accuracy\",\n",
    "        \"energy_kwh\":\"best_tfidf_energy_kwh\",\n",
    "        \"emissions\":\"best_tfidf_co2_kg\"\n",
    "    })\n",
    ")\n",
    "\n",
    "deltas_df.to_csv(\"metrics_deltas_bert_vs_best_tfidf.csv\", index=False)\n",
    "eff_all_models_df.to_csv(\"metrics_joules_per_1pct_all_models_vs_best_tfidf.csv\", index=False)\n",
    "per_example_df.to_csv(\"metrics_energy_normalized.csv\", index=False)\n",
    "best_tfidf_df.to_csv(\"metrics_best_tfidf_per_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "for fn in [\n",
    "    \"metrics_deltas_bert_vs_best_tfidf.csv\",\n",
    "    \"metrics_joules_per_1pct_all_models_vs_best_tfidf.csv\",\n",
    "    \"metrics_energy_normalized.csv\",\n",
    "    \"metrics_best_tfidf_per_dataset.csv\",\n",
    "]:\n",
    "    print(\" -\", fn)\n",
    "\n",
    "print(\"\\nPreview — deltas (BERT vs best TF-IDF):\")\n",
    "display(deltas_df.head())\n",
    "\n",
    "print(\"\\nPreview — J per +1% accuracy (all models vs best TF-IDF):\")\n",
    "display(eff_all_models_df.head())\n",
    "\n",
    "print(\"\\nPreview — energy normalization:\")\n",
    "display(per_example_df.head())\n"
   ],
   "block_group": "37716df8c7b2491da2ea5c8344c08eab",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Saved:\n - metrics_deltas_bert_vs_best_tfidf.csv\n - metrics_joules_per_1pct_all_models_vs_best_tfidf.csv\n - metrics_energy_normalized.csv\n - metrics_best_tfidf_per_dataset.csv\n\nPreview — deltas (BERT vs best TF-IDF):\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "   dataset ref_tfidf_model  bert_accuracy  ref_accuracy  delta_accuracy  \\\n0  ag_news      TF-IDF+SVM       0.950132      0.916447        0.033684   \n1   amazon   TF-IDF+LOGREG       0.947850      0.876400        0.071450   \n2  dbpedia      TF-IDF+SVM       0.992271      0.978314        0.013957   \n\n   bert_energy_kwh  ref_energy_kwh  delta_energy_kwh  bert_co2_kg  ref_co2_kg  \\\n0         0.058533        0.000240          0.058294     0.021607    0.000088   \n1         0.199022        0.000186          0.198837     0.073466    0.000069   \n2         0.277645        0.001308          0.276337     0.102488    0.000483   \n\n   delta_co2_kg  joules_per_1pct_acc_bert_vs_best_tfidf  \n0      0.021518                            62301.278754  \n1      0.073397                           100183.577904  \n2      0.102006                           712763.211228  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>ref_tfidf_model</th>\n      <th>bert_accuracy</th>\n      <th>ref_accuracy</th>\n      <th>delta_accuracy</th>\n      <th>bert_energy_kwh</th>\n      <th>ref_energy_kwh</th>\n      <th>delta_energy_kwh</th>\n      <th>bert_co2_kg</th>\n      <th>ref_co2_kg</th>\n      <th>delta_co2_kg</th>\n      <th>joules_per_1pct_acc_bert_vs_best_tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ag_news</td>\n      <td>TF-IDF+SVM</td>\n      <td>0.950132</td>\n      <td>0.916447</td>\n      <td>0.033684</td>\n      <td>0.058533</td>\n      <td>0.000240</td>\n      <td>0.058294</td>\n      <td>0.021607</td>\n      <td>0.000088</td>\n      <td>0.021518</td>\n      <td>62301.278754</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>amazon</td>\n      <td>TF-IDF+LOGREG</td>\n      <td>0.947850</td>\n      <td>0.876400</td>\n      <td>0.071450</td>\n      <td>0.199022</td>\n      <td>0.000186</td>\n      <td>0.198837</td>\n      <td>0.073466</td>\n      <td>0.000069</td>\n      <td>0.073397</td>\n      <td>100183.577904</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dbpedia</td>\n      <td>TF-IDF+SVM</td>\n      <td>0.992271</td>\n      <td>0.978314</td>\n      <td>0.013957</td>\n      <td>0.277645</td>\n      <td>0.001308</td>\n      <td>0.276337</td>\n      <td>0.102488</td>\n      <td>0.000483</td>\n      <td>0.102006</td>\n      <td>712763.211228</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "\nPreview — J per +1% accuracy (all models vs best TF-IDF):\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "   dataset          model ref_tfidf_model  model_accuracy  ref_accuracy  \\\n0  ag_news      BERT-base      TF-IDF+SVM        0.950132      0.916447   \n1  ag_news     TF-IDF+CNB      TF-IDF+SVM        0.898684      0.916447   \n2  ag_news  TF-IDF+LOGREG      TF-IDF+SVM        0.914868      0.916447   \n3  ag_news     TF-IDF+SVM      TF-IDF+SVM        0.916447      0.916447   \n4   amazon      BERT-base   TF-IDF+LOGREG        0.947850      0.876400   \n\n   delta_accuracy  model_energy_kwh  ref_energy_kwh  delta_energy_kwh  \\\n0        0.033684          0.058533        0.000240          0.058294   \n1       -0.017763          0.000038        0.000240         -0.000202   \n2       -0.001579          0.000196        0.000240         -0.000043   \n3        0.000000          0.000240        0.000240          0.000000   \n4        0.071450          0.199022        0.000186          0.198837   \n\n   joules_per_1pct_acc_vs_best_tfidf  \n0                       62301.278754  \n1                         409.042888  \n2                         986.004412  \n3                                NaN  \n4                      100183.577904  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>model</th>\n      <th>ref_tfidf_model</th>\n      <th>model_accuracy</th>\n      <th>ref_accuracy</th>\n      <th>delta_accuracy</th>\n      <th>model_energy_kwh</th>\n      <th>ref_energy_kwh</th>\n      <th>delta_energy_kwh</th>\n      <th>joules_per_1pct_acc_vs_best_tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ag_news</td>\n      <td>BERT-base</td>\n      <td>TF-IDF+SVM</td>\n      <td>0.950132</td>\n      <td>0.916447</td>\n      <td>0.033684</td>\n      <td>0.058533</td>\n      <td>0.000240</td>\n      <td>0.058294</td>\n      <td>62301.278754</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ag_news</td>\n      <td>TF-IDF+CNB</td>\n      <td>TF-IDF+SVM</td>\n      <td>0.898684</td>\n      <td>0.916447</td>\n      <td>-0.017763</td>\n      <td>0.000038</td>\n      <td>0.000240</td>\n      <td>-0.000202</td>\n      <td>409.042888</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ag_news</td>\n      <td>TF-IDF+LOGREG</td>\n      <td>TF-IDF+SVM</td>\n      <td>0.914868</td>\n      <td>0.916447</td>\n      <td>-0.001579</td>\n      <td>0.000196</td>\n      <td>0.000240</td>\n      <td>-0.000043</td>\n      <td>986.004412</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ag_news</td>\n      <td>TF-IDF+SVM</td>\n      <td>TF-IDF+SVM</td>\n      <td>0.916447</td>\n      <td>0.916447</td>\n      <td>0.000000</td>\n      <td>0.000240</td>\n      <td>0.000240</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>amazon</td>\n      <td>BERT-base</td>\n      <td>TF-IDF+LOGREG</td>\n      <td>0.947850</td>\n      <td>0.876400</td>\n      <td>0.071450</td>\n      <td>0.199022</td>\n      <td>0.000186</td>\n      <td>0.198837</td>\n      <td>100183.577904</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "\nPreview — energy normalization:\n",
     "output_type": "stream"
    },
    {
     "data": {
      "application/vnd.deepnote.dataframe.v3+json": {
       "column_count": 4,
       "columns": [
        {
         "name": "dataset",
         "dtype": "object",
         "stats": {
          "unique_count": 2,
          "nan_count": 0,
          "min": null,
          "max": null,
          "histogram": null,
          "categories": [
           {
            "name": "ag_news",
            "count": 4
           },
           {
            "name": "amazon",
            "count": 1
           }
          ]
         }
        },
        {
         "name": "model",
         "dtype": "object",
         "stats": {
          "unique_count": 4,
          "nan_count": 0,
          "min": null,
          "max": null,
          "histogram": null,
          "categories": [
           {
            "name": "BERT-base",
            "count": 2
           },
           {
            "name": "TF-IDF+CNB",
            "count": 1
           },
           {
            "name": "2 others",
            "count": 2
           }
          ]
         }
        },
        {
         "name": "energy_kwh_per_test_example",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "4.973194968689549e-09",
          "max": "7.701739580284198e-06",
          "histogram": [
           {
            "bin_start": 4.973194968689549E-9,
            "bin_end": 7.746498335002404E-7,
            "count": 3
           },
           {
            "bin_start": 7.746498335002404E-7,
            "bin_end": 1.5443264720317911E-6,
            "count": 0
           },
           {
            "bin_start": 1.5443264720317911E-6,
            "bin_end": 2.314003110563342E-6,
            "count": 0
           },
           {
            "bin_start": 2.314003110563342E-6,
            "bin_end": 3.083679749094893E-6,
            "count": 0
           },
           {
            "bin_start": 3.083679749094893E-6,
            "bin_end": 3.853356387626444E-6,
            "count": 0
           },
           {
            "bin_start": 3.853356387626444E-6,
            "bin_end": 4.623033026157994E-6,
            "count": 0
           },
           {
            "bin_start": 4.623033026157994E-6,
            "bin_end": 5.392709664689545E-6,
            "count": 1
           },
           {
            "bin_start": 5.392709664689545E-6,
            "bin_end": 6.162386303221096E-6,
            "count": 0
           },
           {
            "bin_start": 6.162386303221096E-6,
            "bin_end": 6.9320629417526474E-6,
            "count": 0
           },
           {
            "bin_start": 6.9320629417526474E-6,
            "bin_end": 7.701739580284198E-6,
            "count": 1
           }
          ],
          "categories": null
         }
        },
        {
         "name": "energy_kwh_per_correct_prediction",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "5.533862629874169e-09",
          "max": "8.105971584290251e-06",
          "histogram": [
           {
            "bin_start": 5.533862629874169E-9,
            "bin_end": 8.155776347959119E-7,
            "count": 3
           },
           {
            "bin_start": 8.155776347959119E-7,
            "bin_end": 1.6256214069619498E-6,
            "count": 0
           },
           {
            "bin_start": 1.6256214069619498E-6,
            "bin_end": 2.4356651791279873E-6,
            "count": 0
           },
           {
            "bin_start": 2.4356651791279873E-6,
            "bin_end": 3.245708951294025E-6,
            "count": 0
           },
           {
            "bin_start": 3.245708951294025E-6,
            "bin_end": 4.055752723460063E-6,
            "count": 0
           },
           {
            "bin_start": 4.055752723460063E-6,
            "bin_end": 4.865796495626101E-6,
            "count": 0
           },
           {
            "bin_start": 4.865796495626101E-6,
            "bin_end": 5.675840267792139E-6,
            "count": 1
           },
           {
            "bin_start": 5.675840267792139E-6,
            "bin_end": 6.485884039958177E-6,
            "count": 0
           },
           {
            "bin_start": 6.485884039958177E-6,
            "bin_end": 7.295927812124214E-6,
            "count": 0
           },
           {
            "bin_start": 7.295927812124214E-6,
            "bin_end": 8.105971584290251E-6,
            "count": 1
           }
          ],
          "categories": null
         }
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "row_count": 5,
       "preview_row_count": 5,
       "rows": [
        {
         "dataset": "ag_news",
         "model": "BERT-base",
         "energy_kwh_per_test_example": 7.701739580284198E-6,
         "energy_kwh_per_correct_prediction": 8.105971584290251E-6,
         "_deepnote_index_column": 0
        },
        {
         "dataset": "ag_news",
         "model": "TF-IDF+CNB",
         "energy_kwh_per_test_example": 4.973194968689549E-9,
         "energy_kwh_per_correct_prediction": 5.533862629874169E-9,
         "_deepnote_index_column": 1
        },
        {
         "dataset": "ag_news",
         "model": "TF-IDF+LOGREG",
         "energy_kwh_per_test_example": 2.5839585057802632E-8,
         "energy_kwh_per_correct_prediction": 2.8244045223543796E-8,
         "_deepnote_index_column": 2
        },
        {
         "dataset": "ag_news",
         "model": "TF-IDF+SVM",
         "energy_kwh_per_test_example": 3.152982289273684E-8,
         "energy_kwh_per_correct_prediction": 3.44044011464178E-8,
         "_deepnote_index_column": 3
        },
        {
         "dataset": "amazon",
         "model": "BERT-base",
         "energy_kwh_per_test_example": 4.975558192644673E-6,
         "energy_kwh_per_correct_prediction": 5.249309693142029E-6,
         "_deepnote_index_column": 4
        }
       ],
       "type": "dataframe"
      },
      "text/plain": "   dataset          model  energy_kwh_per_test_example  \\\n0  ag_news      BERT-base                 7.701740e-06   \n1  ag_news     TF-IDF+CNB                 4.973195e-09   \n2  ag_news  TF-IDF+LOGREG                 2.583959e-08   \n3  ag_news     TF-IDF+SVM                 3.152982e-08   \n4   amazon      BERT-base                 4.975558e-06   \n\n   energy_kwh_per_correct_prediction  \n0                       8.105972e-06  \n1                       5.533863e-09  \n2                       2.824405e-08  \n3                       3.440440e-08  \n4                       5.249310e-06  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>model</th>\n      <th>energy_kwh_per_test_example</th>\n      <th>energy_kwh_per_correct_prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ag_news</td>\n      <td>BERT-base</td>\n      <td>7.701740e-06</td>\n      <td>8.105972e-06</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ag_news</td>\n      <td>TF-IDF+CNB</td>\n      <td>4.973195e-09</td>\n      <td>5.533863e-09</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ag_news</td>\n      <td>TF-IDF+LOGREG</td>\n      <td>2.583959e-08</td>\n      <td>2.824405e-08</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ag_news</td>\n      <td>TF-IDF+SVM</td>\n      <td>3.152982e-08</td>\n      <td>3.440440e-08</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>amazon</td>\n      <td>BERT-base</td>\n      <td>4.975558e-06</td>\n      <td>5.249310e-06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "outputs_reference": "s3:deepnote-cell-outputs-production/cf0e6ae5-75ab-4979-b00b-921cd39e3e31",
   "content_dependencies": null
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "deepnote_persisted_session": {
   "createdAt": "2025-09-09T12:25:48.781Z"
  },
  "deepnote_notebook_id": "59d8300cb58a4c53850c0d1d9f02435e"
 }
}
